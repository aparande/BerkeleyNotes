# Nonlinear System Dynamics

Consider the nonlinear system

$$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t).$$

$$f$$ is a vector field which potentially changes with time and governs
how the system evolves.

{% hint style="info" %}

### Definition 30

The system is autonomous if $$f(\boldsymbol{x}, t)$$ is not explicitly dependent on time $$t$$.

{% endhint %}

{% hint style="info" %}

### Definition 31

A point $$x_0$$ is an equilibrium point at time $$t_0$$ if 

$$ \forall t \geq t_0, \ f(\boldsymbol{x}_0, t) = 0 $$

{% endhint %}

Consider a single trajectory $$\phi(t, t_0, \boldsymbol{x}_0)$$.

{% hint style="info" %}

### Definition 32

A set $$S$$ is said to be the $$\omega-$$limit set of $$\phi$$ if

$$ \forall \boldsymbol{y}\in S,\exists t_n\to \infty, \lim_{n\to\infty}\phi(t_n, t_0, \boldsymbol{x}_0) = \boldsymbol{y} $$

{% endhint %}

Whereas linear systems converge to a single point if they converge at
all, nonlinear systems can converge to a set of points. Thus the
$$\omega-$$limit set essentially generalizes the idea of a limit.

{% hint style="info" %}

### Definition 33

A set $$M\subset \mathbb{R}^n$$ is said to be invariant if 

$$ \forall t\geq t_0,\ \boldsymbol{y}\in M \implies \phi(t, t_0, \boldsymbol{y}) \in M $$

{% endhint %}

An invariant set is one which a trajectory of the system will never
leave once it enters the set. Just like linear systems, non-linear
systems can also have periodic solutions.

{% hint style="info" %}

### Definition 34

A closed orbit $$\gamma$$ is a trajectory of the system such that $$\gamma(0) = \gamma(T)$$ for finite $$T$$.

{% endhint %}

## Solutions to Nonlinear Systems

Consider the nonlinear system

$$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t),\qquad \boldsymbol{x}(t_0) = \boldsymbol{x}_0\in \mathbb{R}^n.$$

{% hint style="info" %}

### Definition 35

A function $$\boldsymbol{\Phi}(t)$$ is a solution to $$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t),\ \boldsymbol{x}(t_0) = \boldsymbol{x}_0$$ on the closed interval $$[t_0, t]$$ if $$\boldsymbol{\Phi}(t)$$
is defined on the interval $$[t_0, t]$$, $$\frac{d^{}\boldsymbol{\Phi}}{dt^{}} = f(\boldsymbol{\Phi}(t), t)$$ on the interval $$[t_0, t]$$, and $$\boldsymbol{\Phi}(t_0) = \boldsymbol{x}_0$$.

{% endhint %}

We say that $$\boldsymbol{\Phi}(t)$$ is a solution in the sense of
Caratheodory if

$$\boldsymbol{\Phi}(t) = \boldsymbol{x}_0 + \int_{t_0}^t f(\boldsymbol{\Phi}(\tau), \tau)d\tau.$$

Because the system is nonlinear, it could potentially have no solution,
one solution, or many solutions. These solutions could exist locally, or
they could exist for all time. We might also want to know when there is
a solution which depends continuously on the initial conditions.

{% hint style="info" %}

### Theorem 7 (Local Existence and Uniqueness) {#theorem-7}

Given $$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t),\ \boldsymbol{x}(t_0) = \boldsymbol{x}_0\in\mathbb{R}^n$$ where
$$f$$ is piecewise continuous in $$t$$ and $$\exists T>t_0$$ such that $$\forall t\in [t_0, T], f$$ is $$L$$-Lipschitz Continuous, then $$\exists \delta > 0$$ such that a
solution exists and is unique $$\forall t\in [t_0, t_0 + \delta]$$.

{% endhint %}

can be proved using the Contraction Mapping Theorem (Theorem 2) by
finding $$\delta$$ such that the function
$$P:C_n[t_0, t_0+\delta] \to C_n[t_0, t_0+\delta]$$ given by

$$P(\boldsymbol{\Phi})(t) = \boldsymbol{x}_0 + \int_{t_0}^{t_0+\delta} f(\boldsymbol{\Phi}(\tau),     \tau)d\tau$$

is a contraction under the norm
$$\|\boldsymbol{\Phi}\|_\infty = \sup_{t_0\leq t \leq t_0+\delta} \|\boldsymbol{\Phi}(t)\|$$.

{% hint style="info" %}

### Theorem 8 (Global Existence and Uniqueness) {#theorem-8}

Suppose $$f(\boldsymbol{x}, t)$$ is piecewise continuous in $$t$$ and $$\forall T\in [t_0, \infty)$$, $$\exists L_T < \infty$$ such that $$f$$ is $$L_T$$ Lipshitz continuous
for all $$\boldsymbol{x}, \boldsymbol{y} \in \mathbb{R}^n$$, then the nonlinear system  has exactly one
solution on $$[t_0, T]$$.

{% endhint %}

Once we know that solutions to a nonlinear system exist, we can
sometimes bound them.

{% hint style="info" %}

### Theorem 9 (Bellman-Gronwall Lemma) {#theorem-9}

Suppose $$\lambda\in\mathbb{R}$$ is a constant and $$\mu:[a,b]\to\mathbb{R}$$ is continuous and
non-negative, then for a continuous function $$y:[a, b]\to\mathbb{R}$$ 

$$ y(t) \leq \lambda + \int_a^t \mu(\tau)y(\tau)d\tau \implies y(t) \leq \lambda \text{exp}\left(\int_a^t\mu(\tau)d\tau\right) $$

{% endhint %}

Another thing we might want to do is understand how the nonlinear system
reacts to changes in the initial condition.

{% hint style="info" %}

### Theorem 10

Suppose the system $$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t),\ \boldsymbol{x}(t_0) = \boldsymbol{x}_0$$
satisfies the conditions of global uniqueness and existence. Fix $$T\in[t_0, \infty]$$ and suppose $$\boldsymbol{x}(\cdot)$$ and $$\boldsymbol{z}(\cdot)$$  are two solutions
satisfying $$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x}, t), \boldsymbol{x}(t_0) = \boldsymbol{x}_0$$ and
$$\frac{d^{}\boldsymbol{z}}{dt^{}} = f(\boldsymbol{z}(t), t),\ \boldsymbol{z}(t_0)=\boldsymbol{z}_0$$, then $$\forall \epsilon > 0, \exists \delta > 0$$ such that 

$$ \|\boldsymbol{x}_0 - \boldsymbol{z}_0\| < \delta \implies \|\boldsymbol{x} - \boldsymbol{z}\|_{\infty} < \epsilon. $$

{% endhint %}

is best understood by defining a function
$$\Psi:\mathbb{R}^n \to C_n[t_0, t]$$ where
$$\Psi(\boldsymbol{x}_0)(t)$$ returns the solution to the system given
the initial condition. If the conditions of are satisfied, then the
function $$\Psi$$ will be continuous.

## Planar Dynamical Systems

Planar dynamical systems are those with 2 state variables. Suppose we
linearize the autonomous system
$$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x})$$ at an
equilibrium point.

$$\frac{d^{}\boldsymbol{x}}{dt^{}} = \frac{\partial f}{\partial \boldsymbol{x}} \bigg\lvert_{\boldsymbol{x_0}}\boldsymbol{x}$$

Depending on the eigenvalues of
$$\frac{\partial f}{\partial \boldsymbol{x}}$$, the Jacobian, we get
several cases for how this linear system behaves. Weâ€™ll let $$z_1$$ and
$$z_2$$ be the eigenbasis of the *phase space*.

1.  The eigenvalues are real, yielding solutions
    $$z_1 = z_1(0)e^{\lambda_1         t}, z_2 = z_2(0)e^{\lambda_2 t}$$.
    If we eliminate the time variable, we can plot the trajectories of
    the system.

    $$\frac{z_1}{z_1(0)} = \left(\frac{z_2}{z_2(0)}\right)^{\frac{\lambda_1}{\lambda_2}}$$

    1.  When $$\lambda_1, \lambda_2 < 0$$, all trajectories converge to
        the origin, so we call this a **stable node**.

    2.  When $$\lambda_1, \lambda_2 > 0$$, all trajectories blow up, so
        we call this an **unstable node**.

    3.  When $$\lambda_1 < 0 < \lambda_2$$, the trajectories will
        converge to the origin along the axis corresponding to
        $$\lambda_1$$ and diverge along the axis corresponding to
        $$\lambda_2$$, so we call this a **saddle node**.

2.  There is a single repeated eigenvalue with one eigenvector. As
    before, we can eliminate the time variable and plot the trajectories
    on the $$z_1$$, $$z_2$$ axes.

    1.  When $$\lambda < 0$$, the trajetories will converge to the
        origin, so we call it an **improper stable node**

    2.  When $$\lambda > 0$$, the trajetories will diverge from the
        origin, so we call it an **improper unstable node**

3.  When there is a complex pair of eigenvalues, the linear system will
    have oscillatory behavior. The Real Jordan form of
    $$\frac{\partial f}{\partial \boldsymbol{x}}$$ will look like

    $$\frac{\partial f}{\partial \boldsymbol{x}} = \begin{bmatrix} \alpha & \beta \\ -\beta & \alpha \end{bmatrix}.$$

    The parameter $$\beta$$ will determine the direction of the
    trajectories (clockwise if positive).

    1.  When $$\alpha < 0$$, the trajectories will spiral towards the
        origin, so we call it a **stable focus**.

    2.  When $$\alpha = 0$$, the trajectories will remain at a constant
        radius from the origin, so we call it a **center**.

    3.  When $$\alpha > 0$$, the trajectories will spiral away from the
        origin, so we call it an **unstable focus**.

It turns out that understanding the linear dynamics at equilibrium
points can be helpful in understanding the nonlinear dynamics near
equilibrium points.

{% hint style="info" %}

### Theorem 11 (Hartman-Grobman Theorem) {#theorem-11}

If the linearization of a planar dynamical system $$\frac{d^{}\boldsymbol{x}}{dt^{}} = f(\boldsymbol{x})$$ at an equilibrium point $$\boldsymbol{x_0}$$ has no zero or purely imaginary eigenvalues, then there exists a
homeomorphism from a neighborhood $$U$$ of $$\boldsymbol{x}_0$$ into $$\mathbb{R}^2$$ which takes
trajectories of the nonlinear system and maps them onto the linearization
where $$h(\boldsymbol{x_0}) = 0$$, and the homeomorphism can be chosen to preserve the
parameterization by time.

{% endhint %}

essentially says that the linear dynamics predict the nonlinear dynamics
around equilibria, but only for a neighborhood around the equilibrium
point. Outside of this neighborhood, the linearization may be very
wrong.

Suppose that we have a simply connected region $$D$$ (meaning $$D$$
cannot be contracted to a point) and we want to know if it contains a
closed orbit.

{% hint style="info" %}

### Theorem 12 (Bendixon's Theorem) {#theorem-12}

If $$\text{div}(f)$$ is not identically zero in a sub-region of $$D$$ and does not
change sign in $$D$$, then $$D$$contains no closed orbits.

{% endhint %}

lets us rule out closed orbits from regions of $$\mathbb{R}^2$$. If we
have a positively invariant region, then we can determine whether it
contains closed orbits.

{% hint style="info" %}

### Theorem 13 (Poincare-Bendixson Theorem) {#theorem-13}

If $$M$$ is a compact, positively invariant set for the flow $$\phi_t(\boldsymbol{x})$$,
then if $$M$$ contains no equilibrium points, then $$M$$has a limit cycle.

{% endhint %}

