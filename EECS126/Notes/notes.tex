\input{../../header.tex}
\begin{document}
\title{EECS126 Course Notes}
\author{Anmol Parande}
\date{Spring 2021 - Professor Thomas Courtade}
\maketitle
\textbf{Disclaimer: }These notes reflect 126 when I took the course (Spring 2021). They may not accurately reflect current course content, so use at your own risk.
If you find any typos, errors, etc, please raise an issue on the \href{https://github.com/parandea17/BerkeleyNotes}{GitHub repository}.
\tableofcontents
\newpage
\section{Introduction to Probability}
\begin{definition}
  A probability space is a triple $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a set of objects called the sample space, $\mathcal{F}$ is a family of subsets of $\Omega$ called events, and the probability measure $P:\mathcal{F}\rightarrow [0,1]$.
  \label{defn:prob-space}
\end{definition}
One key assumption we make is that $\mathcal{F}$ is a $\sigma$-algebra containing $\Omega$, meaning that countably many complements, unions, and intersections of events in $\mathcal{F}$ are also events in $\mathcal{F}$.
The probability measure $P$ must obey \textbf{Kolmogorov's Axioms}.
\begin{enumerate}
  \item $\forall A \in \mathcal{F},\ P(A) \geq 0$
  \item $P(\Omega) = 1$
  \item If $A_1, A_2, \cdots\in \mathcal{F}$ and $\forall i\ne j,\ A_i\bigcap A_j=\emptyset$, then $P\left(\bigcup_{i\geq 1}A_i\right) = \sum_{i\geq1}P(A_i)$
\end{enumerate}
We choose $\Omega$ and $\mathcal{F}$ to model problems in a way that makes our calculations easy.
\begin{theorem}
  \[
	P(A^c) = 1 - P(A)
  \]
  \label{thm:complementarity}
\end{theorem}
\begin{theorem}[Inclusion-Exclusion Principle]
  \[
	P\left( \bigcup_{i=1}^{n}A_i \right) = \sum_{k=1}^{n}(-1)^{k+1}\left( \sum_{1\leq i_1<\cdots<i_k\leq n} P(A_{i_1}\cap \cdots \cap A_{i_k}) \right)
  \]
  \label{thm:inclusion-exclusion}
\end{theorem}
\begin{theorem}[Law of Total Probability]
  If $A_1, A_2, \cdots$ partition $\Omega$ (i.e $A_i$ are disjoint and $\cup A_i = \Omega$), then for event $B$,
  \[
	P(B) = \sum_iP(B\cap A_i)
  \]
  \label{thm:total-prob}
\end{theorem}
\subsection{Conditional Probability}
\begin{definition}
  If $B$ is an event with $P(B)>0$, then the conditional probability of $A$ given $B$ is
  \[
	P(A|B) = \frac{P(A\cap B)}{P(B)}
  \]
  \label{defn:conditional-prob}
\end{definition}
Intuitively, conditional probabilty is the probability of event $A$ given that event $B$ has occurred.
In terms of probability spaces, it is as if we have taken $(\Omega, \mathcal{F}, P)$ and now have a probabilty measure $P(\cdot|C)$ belonging to the space $(\Omega, \mathcal{F}, P(\cdot|C))$.
\begin{theorem}[Bayes Theorem]
  \[
	P(A|B) = \frac{P(B|A)P(A)}{P(B)}
  \]
  \label{thm:bayes}
\end{theorem}
\subsection{Independence}
\begin{definition}
  Events $A$ and $B$ are independent if $P(A\cap B) = P(A)P(B)$
  \label{defn:independence}
\end{definition}
If $P(B)>0$, then $A, B$ are independent if and only if $P(A|B) = P(A)$. In other words, knowing $B$ occurred gave no extra information about $A$.
\begin{definition}
  If $A,B,C$ with $P(C)>0$ satisfy $P(A\cap B|C) = P(A|C)P(B|C)$, then $A$ and $B$ are conditionally independent given $C$.
  \label{defn:conditional-independence}
\end{definition}
Conditional independence is a special case of independence where $A$ and $B$ are not necessarily independent in the original probability space which has the measure $P$, but are independent in the new probability space conditioned on $C$ with the measure $P(\cdot|C)$.
\section{Random Variables and their Distributions}
\begin{definition}
  A random variable is a function $X:\Omega\rightarrow\mathbb{R}$ with the property $\forall \alpha\in\mathbb{R},\ \{\omega\in\Omega:\ X(\omega) \leq \alpha\} \in \mathcal{F}$.
  \label{defn:rv}
\end{definition}
The condition in \cref{defn:rv} is necessary to compute $P(X\leq \alpha),\ \forall \alpha\in\mathbb{R}$.
This requirement also let us compute $P(X\in B)$ for most sets by leveraging the fact that $\mathcal{F}$ is closed under complements, unions, and intersections.
For example, we can also compute $P(X > \alpha)$ and $P(\alpha < X \leq \beta)$.
In this sense, the property binds the probability space to the random variable.

\cref{defn:rv} also implies that random variables satisfy particular algebraic properties.
For example, if $X,Y$ are random variables, then so are $X+Y, XY, X^p, \lim_{n\to\infty}X_n$, etc.
\begin{definition}
  A discrete random variable is a random variable whose codomain is countable.
  \label{defn:drv}
\end{definition}
\begin{definition}
  A continuous random variable is a random variable whose codomain is the real numbers.
  \label{defn:crv}
\end{definition}
Although random variables are defined based on a probability space, it is often most natural to model problems without explicitly specifying the probability space.
This works so long as we specify the random variables and their distribution in a ``consistent'' way. 
This is formalized by the so-called \href{https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem}{Kolmogorov Extension Theorem} but can largely be ignored.
\subsection{Distributions}
Roughly speaking, the distribution of a random variable gives an idea of the likelihood that a random variable takes a particular value or set of values.
\begin{definition}
  The probability mass function (or distribution) of a random variable $X$ is the frequency with which $X$ takes on different values.
  \[
	p_X:\mathcal{X} \rightarrow [0, 1] \text{ where } \mathcal{X} = \text{range}(X),\qquad p_X(x) = \pr{X=x}.
  \]
  \label{defn:pmf}
\end{definition}
Note that $\sum_{x\in\mathcal{X}}p_X(x) = 1$ since $\bigcap_{x\in\mathcal{X}}\{w: X(w) = x\} = \Omega$.

Continuous random variables are largely similar to discrete random variables.
One key difference is that instead of being described by a probability ``mass'', they are instead described by a probability ``density''.
\begin{definition}
  The probability density function (distribution) of a continuous random variable describes the density by which a random variable takes a particular value.
  \[
	f_X: \mathbb{R}\to [0, \infty) \text{ where } \int_{-\infty}^{\infty}f_X(x)dx = 1 \text{ and } \pr{X\in B} = \int_B f_X(x)dx
  \]
  \label{defn:pdf}
\end{definition}
Observe that if a random variable $X$ is continuous, then the probability that it takes on a particular value is zero.
\[
  \pr{X=x} = \lim_{\delta\to0} \pr{x \leq X \leq x +\delta} = \lim_{\delta\to 0}\int_x^{x+\delta}f_X(u)du = \int_{x}^{x}f_X(u)du = 0
\]
\begin{definition}
  The cumulative distribution function (CDF) gives us the probability of a random variable $X$ being less than or equal to a particular value.
  \[
	F_X:\mathbb{R} \to [0, 1],\quad F_X(x) = \pr{X \leq x}
  \]
  \label{defn:cdf}
\end{definition}
Note that by the Kolomogorov axioms, $F_X$ must satisfy three properties:
\begin{enumerate}
  \item $F_X$ is non-decreasing.
  \item $\lim_{x\to0} F_X(x) = 0$ and $\lim_{x\to\infty} F_X(x) = 1$.
  \item $F_X$ is right continuous.
\end{enumerate}
It turns out that if we have any function $F_X$ that satisfies these three properties, then it is the CDF of some random variable on some probability space.
Note that $F_X(x)$ gives us an alternative way to define continuous random variables.
If $F_X(x)$ is absolutely continuous, then it can be expressed as
\[
  F_X(x) = \int_{-\infty}^{x}f_X(x)dx
\]
for some non-negative function $f_X(x)$, and this is the PDF of a continuous random variable.

Often, when modeling problems, there are multiple random variables that we want to keep track of.
\begin{definition}
  If $X$ and $Y$ are random variables on a common probability space $(\Omega, \mathcal{F}, P)$, then the joint distribution (denoted $p_XY(x, y)$ or $f_{XY}(x, y)$ describes the frequencies of joint outcomes.
  \label{defn:joint-dist}
\end{definition}
Note that it is possible for $X$ to be continuous and $Y$ to be discrete (or vice versa).
\begin{definition}
  The marginal distribution of a joint distribution is the distribution of a single random variable.
  \[
	p_X(x) = \sum_yp_{XY}(x, Y=y), \qquad f_X(x) = \int_{-\infty}^{\infty}f_{XY}(x, y)dy
  \]
  \label{defn:marginal-dist}
\end{definition}
\begin{definition}
  Two random variables $X$ and $Y$ are independent if their joint distribution is the product of the marginal distributions.
  \label{defn:rv-independence}
\end{definition}
Just like independence, we can extend the notion of conditional probability to random variables.
\begin{definition}
  The conditional distribution of $X$ given $Y$ captures the frequencies of $X$ given we know the value of $Y$.
  \[
	p_{X|Y}(x|y) = \frac{P_{XY}(x, y)}{p_Y(y)}, \qquad f_{X|Y}(x|y) = \frac{f_{XY}(x, y)}{f_Y(y)}
  \]
  \label{defn:conditional-dist}
\end{definition}
Often, we need to combine or transform several random variables.
A derived distribution is the obtained by arithmetic of several random variables or applying a function to several (or many) random variables.
Since the CDF of a distribution essentially defines that random variable, it can often be easiest to work backwards from the CDF to the PDF or PMF.
In the special case where we want to find $Y=g(X)$ for a function $g$.
\[
  F_y(y) =\pr{Y \leq y} = \pr{g(x) \leq y} = \pr{X \in g^{-1}([-\infty, y])}, \quad g^{-1}(y) = \{ x: g(x) = y \}.
\]
Another special case of a derived distribution is when adding random variables together.
\begin{theorem}
  The resulting distribution of a sum of two independent random variables is the convolution of the distributions of the two random variables.
  \[
	p_{X+Y}(z) = \sum_{k=-\infty}^{\infty}p_X(k)p_Y(z-k), \quad f_{X+Y}(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)dx
  \]
  \label{thm:convolution}
\end{theorem}
\subsection{Properties of Distributions}
\subsubsection{Expectation}
\begin{definition}
  The expectation of a random variable describes the center of a distribution,
  \[
	\expect{X}=\sum_{x\in\mathcal{X}}xp_X(x), \quad \expect{X} = \int_{-\infty}^{\infty}xf_X(x)dx
  \]
  provided the sum or integral converges.
  \label{defn:drv-expectation}
\end{definition}
Expectation has several useful properties.
If we want to compute the expectation of a function of a random variable, then we can use the law of the unconscious statisitician.
\begin{theorem}[Law of the Unconscious Statistician]
  \[
	\expect{g(X)} = \sum_{x\in\mathcal{X}}g(x)p_X(x), \quad \expect{g(X)} = \int_{-\infty}^{\infty}g(x)f_X(x)dx
  \]
  \label{thm:unconscious-statistician}
\end{theorem}
Another useful property is its linearity.
\[
  \expect{aX+bY} = a\expect{X}+b\expect{Y},\ \forall a, b\in\mathbb{R}.
\]

Sometimes it can be difficult to compute expectations directly. For disrete distributions, we can use the tail-sum formula.
\begin{theorem}[Tail Sum]
  For a non-negative integer random variable,
  \[
	\expect{X} = \sum_{k=1}^{\infty}\pr{X\geq k}.
  \]
  \label{thm:tail-sum}
\end{theorem}
When two random variables are independent, expectation has some additional properties.
\begin{theorem}
  If $X$ and $Y$ are independent, then
  \[
	\expect{XY} = \expect{X}\expect{Y}.
  \]
  \label{thm:indep-expect}
\end{theorem}
We can apply expectations to conditional distributions as well.
\begin{definition}
  The conditional expectation of a conditional distribution is given by
  \[
	\expect{X|Y=y} = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y), \quad \expect{X|Y=y} = \int_{-\infty}^{\infty}xf_{X|Y}(x, y)dx
  \]
  \label{defn:drv-conditional-expect}
\end{definition}
Notice that $\expect{X|Y}$ is a function of the random variable $Y$, meaning we can apply \cref{thm:unconscious-statistician}.
\begin{theorem}[Tower Property]
  For all functions $f$,
  \[
	\expect{f(Y)X} = \expect{f(Y)\expect{X|Y}}
  \]
  \label{thm:tower-property}
\end{theorem}
If we apply \cref{thm:tower-property} to the function $f(Y) = 1$, then we can see that $\expect{\expect{X|Y}} = \expect{X}$.

Earlier, we saw that we find a derived distribution by transforming and combining random variables.
Sometimes, we don't need to actually compute the distribution, but only some of its properties.
\begin{definition}
  The nth moment of a random variable is $\expect{X^n}$.
  \label{defn:moment}
\end{definition}
It turns out that we can encode the moments of a distribution into the coefficients of a special power series.
\begin{definition}
  The moment generating function of a random variable $X$ is given by $M_X(t) = \expect{e^{tX}}$.
  \label{defn:mgf}
\end{definition}
Notice that if we apply the power series expansion of $e^{tX}$, we see that \[
  M_X(t) = \sum_{n=0}^{\infty}\frac{t!}{n!}\expect{X^n}.
\]
Thus the nth moment is encoded in the coefficients of the power series and we can retrieve them by taking a derivative:
\[
  \expect{X^n} = \diff[n]{}{t}M_X(t).
\]
Another interesting point to notice is that for a continuous random variable
\[
  M_X(t) = \int_{-\infty}^{\infty}f_X(x)e^{tx}dx
\]
is the Laplace transform of the distribution over the real line,
and for a discrete random variable,
\[
  M_X(t) = \sum_{x=-\infty}^{\infty}p_X(x)e^{tx}
\]
is the Z-transform of the distribution evaluated along the curve at $e^{-t}$.
\begin{theorem}
  If the MGF of a function exists, then it uniquely determines the distribution.
  \label{thm:mgf}
\end{theorem}
This provides another way to compute the distribution for a sum of random variables because we can just multiply their MGF.
\subsubsection{Variance}
\begin{definition}
  The variance of a discrete random variable $X$ describes its spread around the expectation and is given by
  \[
	\var{X} = \expect{(X-\expect{X})^2} = \expect{X^2}-\expect{X}^2.
  \]
  \label{defn:drv-variance}
\end{definition}
\begin{theorem}
  When two random variables $X$ and $Y$ are independent, then 
  \[
	\var{X+Y} = \var{X} + \var{Y}.
  \]
  \label{defn:indep-var}
\end{theorem}
\begin{definition}
  The covariance of two random variables describes how much they depend on each other and is given by
  \[
	\cov{X, Y} = \expect{(X-\expect{X})(Y-\expect{Y})} = \expect{XY} - \expect{X}\expect{Y}.
  \]
  \label{defn:cov}
\end{definition}
If $\cov{X,Y} = 0$ then $X$ and $Y$ are uncorrelated.
\begin{definition}
  The correlation coefficient gives a single number which describes how random variables are correlated.
  \[
	\rho(X, Y) = \frac{\cov{X, Y}}{\sqrt{\var{X}}\sqrt{\var{Y}}}.
  \]
  \label{defn:correlation}
\end{definition}
Note that $-1\leq \rho \leq 1$.
Just as expectation can change when we know additional information, so can variance.
\begin{definition}
  Conditional Variance is the variance of $X$ given the value of $Y$.
  \[
	\var{X|Y=y} = \expect{(X - \expect{X|Y=y})^2 | Y=y} = \expect{X^2|Y=y} - \expect{X|Y=y}^2
  \]
  \label{defn:conditional-var}
\end{definition}
Conditional variance is a random variable just as expectation is.
\begin{theorem}[Law of Total Variance]
  \[
	\var{X} = \expect{\var{X|Y}} + \var{\expect{X|Y}}
  \]
  \label{thm:total-variance}
\end{theorem}
The second term in the law of total variance ($\var{\expect{X|Y}}$) can be interpreted as on average, how much uncertainty there is in $X$ given we know $Y$.
\subsection{Common Distributions}
\subsubsection{Discrete Distributions}
\begin{definition}
  $X$ is uniformly distributed when each value of $X$ has equal probability.
  \[
	X\sim \text{Uniform}(\{ 1, 2, \cdots, n \}) \implies p_X(x)  = \begin{cases}
	  \frac{1}{n} & x = 1, 2, \cdots, n,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:drv-uniform}
\end{definition}
\begin{definition}
  $X$ is a Bernoulli random variable if it is either $0$ or $1$ with $p_X(1) = p$.
  \[
	X\sim\text{Bernoulli}(p) \implies p_X(x) = \begin{cases}
	  1 - p & x=0,\\
	  p & x=1,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = p \qquad \var{X} = (1-p)p
  \]
  \label{defn:bernoulli}
\end{definition}
Bernoulli random variables are good for modeling things like a coin flip where there is a probability of success.
Bernoulli random variables are frequently used as indicator random variables $\mathbbm{1}_A$ where
\[
  \mathbbm{1}_A = \begin{cases}
	1 & \text{if A occurs,}\\
	0 & \text{ else.}
  \end{cases}
\]
When paired with the linearity of expectation, this can be a powerful method of computing the expectation of something.
\begin{definition}
  $X$ is a Binomial random variable when
  \[
	X \sim \text{Binomial}(n, p) \implies p_X(x) = \begin{cases}
	\binom{n}{x} p^x (1-p)^{n-x} & x=0, 1, \cdots, n\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = np \qquad \var{X} = np(1-p)
  \]
  \label{defn:binomial}
\end{definition}
A binomial random variable can be thought of as the number of successes in $n$ trials. 
In other words, 
\[
  X \sim \text{Binomial}(n, p) \implies X = \sum_{i=1}^{n}X_i, \quad X_i \sim \text{Bernoulli}(p).
\]
By construction, if $X\sim\text{Binomial}(n, p)$ and $Y\sim\text{Binomial}(m, p)$ are independent, then $X+Y \sim \text{Binomial}(m+n, p)$.
\begin{definition}
  A Geometric random variable is distributed as
  \[
	X\sim\text{Geom}(p) \implies p_X(x) = \begin{cases}
	  p(1-p)^{x-1} & x=1, 2, \cdots\\
	  0 & \text{ else}.
	\end{cases}
  \]
  \[
	\expect{X} = \frac{1}{p} \qquad \var{X} = \frac{1-p}{p^2}
  \]
  \label{defn:geometric}
\end{definition}
Geometric random variables are useful for modeling the number of trials required before the first success.
In other words,
\[
  X \sim \text{Geom}(p) \implies X = \min\{k \geq 1: X_k=1 \} \text{ where } X_i\sim \text{Bernoulli}(p).
\]
A useful property of geometric random variables is that they are memoryless:
\[
  \pr{X=K+M|X>k} = \pr{X=M}.
\]
\begin{definition}
  A Poisson random variable is distributed as
  \[
	X\sim Poisson(\lambda) \implies p_X(x) = \begin{cases}
	  \frac{\lambda^xe^{-\lambda}}{x!} & x=0, 1, \cdots \\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = \lambda
  \]
  \label{defn:poisson}
\end{definition}
Poisson random variables are good for modeling the number of arrivals in a given interval.
Suppose you take a given time interval and divide it into $n$ chunks where the probability of arrival in chunk $i$ is $X_i \sim \text{Bernoulli}(p_n)$.
Then the total number of arrivals $X_n = \sum_{i=1}^{n}X_i$ is distributed as a Binomial random variable with expectation $np_n=\lambda$.
As we increase $n$ to infinity but keep $\lambda$ fixed, we arrive at the poisson distribution.

A useful fact about Poisson random variables is that if $X\sim\text{Poisson}(\lambda)$ and $Y\sim\text{Poisson}(\mu)$ are independent, then $X+Y \sim \text{Poisson}(\lambda + \mu)$.
\subsubsection{Continuous Distributions}
\begin{definition}
  A continuous random variable is uniformly distributed when the pdf of $X$ is constant over a range.
  \[
	X \sim \text{Uniform}(a, b) \implies f_X(x) = \begin{cases}
	  \frac{1}{b-a} & a \leq x \leq b,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:crv-uniform}
\end{definition}
The CDF of a uniform distribution is given by
\[
  F_X(x) = \begin{cases}
	0, &  x < a,\\
	\frac{x-a}{b-a}, & x\in[a, b)\\
	1, & x \geq b.
  \end{cases}
\]
\begin{definition}
  A continuous random variable is exponentially distributed when its pdf is given by
  \[
	X \sim \text{Exp}(\lambda) \implies f_X(x) = \begin{cases}
	  \lambda e^{-\lambda x} & x \geq 0,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:crv-exp}
\end{definition}
Exponential random variables are the only continuous random variable to have the memoryless property:
\[
  \pr{X > t+s | X > s} = \pr{X > t}, \quad t \geq 0.
\]
The CDF of the exponential distribution is given by
\[
  F_X(x) = \lambda \int_0^{x}e^{-\lambda u}du = 1 - e^{-\lambda x}
\]
\begin{definition}
  $X$ is a Gaussian Random Variable with mean $\mu$ and variance $\sigma^2$ (denoted $X\sim \mathcal{N}(\mu, \sigma^2)$) if it has the PDF
  \[
	f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
  \]
  \label{defn:normal-rv}
\end{definition}
The standard normal is $X\sim\mathcal{N}(0, 1)$, and it has the CDF
\[
  \Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{\frac{-u^2}{2}} du
\]
There is no closed from for $\Phi(x)$.
It turns out that every normal random variable can be transformed into the standard normal (i.e $\frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)$).
Some facts about Gaussian random variables are
\begin{enumerate}
  \item If $X\sim\mathcal{N}(\mu_x, \sigma_x^2),\ Y\sim\mathcal{N}(\mu_y, \sigma_y^2)$ are independent, then $X+Y \sim \mathcal{N}(\mu_x+\mu_y, \sigma_x^2 + \sigma_y^2)$.
  \item If $X,Y$ are independent and $(X+Y), (X-Y)$ are independent, then both $X$ and $Y$ are Gaussian with the same variance.
\end{enumerate}
\section{Concentration Inequalities}
In real life, for the most part, we can't compute probabilities in closed form.
Instead, we either bound them, or we want to show that $P(A) \approx 0$ or $P(A) \approx 1$.
\begin{theorem}[Markov's Inequality]
  For a non-negative random variable $X$,
  \[
	\pr{X \geq t} \leq \frac{\expect{X}}{t}, \quad t \geq 0.
  \]
  \label{thm:markov}
\end{theorem}
\begin{theorem}[Chebyshev's Inequality]
  If $X$ is a random variable, then
  \[
	\pr{|X - \expect{X}| \geq t} \leq \frac{\var{X}}{t^2}.
  \]
  \label{thm:chebyshev}
\end{theorem}
Intuitively, \cref{thm:chebyshev} gives gives a ``better'' bound than \cref{thm:markov} because it incorporates the variance of the random variable.
Using this idea, we can define an even better bound that incorporates information from all moments of the random variable.
\begin{definition}[Chernoff Bound]
	For a random variable $X$ and $a\in\mathbb{R}$,\[
		\pr{X \geq a} \leq \frac{\expect{e^{tX}}}{e^{ta}} = e^{-ta}M_x(t).
	\]
	\label{defn:chernoff}
\end{definition}
After computing the Chernoff bound for a general $t$, we can then optimize over it to compute the best bound possible.
\section{Convergence}
The idea of convergence brings the mathematical language of limits into probability.
The fundamental question we want to answer is given random variables $X_1, X_2, \cdots$, what does it mean to compute
\[
	\lim_{n\to\infty}X_n.
\]
This question is not as straightforward as it seems because random variables are functions, and there are many ways to define the convergence of functions.
\begin{definition}
	A sequence of random variables converges almost surely to $X$ if \[
		P\left(\lim_{n\to \infty}X_n = X\right) = 1
	\]
	\label{defn:almost-sure}
\end{definition}
One result of almost sure convergence deals with deviations around the mean of many samples.
\begin{theorem}[Strong Law of Large Numbers]
	If $X_1, X_2, \cdots, X_n$ are independently and identically distributed to $X$ where $\expect{X} < \infty$, then $\frac{1}{n}\sum_i X_i$ converges almost surely to $\expect{X}$.
	\label{thm:strong-law}
\end{theorem}
The strong law tells us that for any observed realization, there is a point after which there are no deviations from the mean.
\begin{definition}
	A sequence of random variables converges in probability if \[
		\forall \epsilon > 0, \quad \lim_{n\to\infty}P(|X_n - X| > \epsilon) = 0
	\]
	\label{defn:converge-prob}
\end{definition}
Convergence in probability can help us formalize the intuition that we have which says probability is the frequency with which an even happens over many trials of an event.
\begin{theorem}[Weak Law of Large Numbers]
  Let $X_1, X_2, \cdots, X_n$ be independently and identically distributed according to $X$, and let $M_n = \frac{1}{n}\sum X_i$.
  Then for $\epsilon > 0$, \[
	\lim_{n\to\infty} \pr{|M_n - \expect{X}| > \epsilon} = 0.
  \]
  \label{thm:weak-law}
\end{theorem}
It tells us that the probability of a deviation of $\epsilon$ from the true mean will go to 0 in the limit, but we can still observe these deviations.
Nevertheless, the weak law helps us formalize our intuition about probability.
If $X_1, X_2, \cdots, X_n$ are independently and identically distributed according to $X$, then we can define the empirical frequency
\[
  F_n = \frac{\sum\mathbbm{1}_{X_i\in B}}{n} \implies \expect{F_n} = P(X \in B).
\]
By \cref{thm:weak-law}, \[
  \lim_{n\to\infty}\pr{|F_n - P(X\in B)| > \epsilon} = 0,
\]
meaning over many trials, the empirical frequency is equal to the probility of the event, matching intuition.
\begin{definition}
	A sequence of random variables converges in distribution if \[
		\lim_{n\to\infty}F_{X_n}(x) = F_x(x).
	\]
	\label{defn:converge-dist}
\end{definition}
An example of convergence in distribution is the central limit theorem.
\begin{theorem}[Central Limit Theorem]
	If $X_1, X_2, \cdots$ are independently and identically distributed according to $X$ with $\var{X} = \sigma^2$ and $\expect{X} = \mu$, then
	\[
		\lim_{n\to\infty}P\left(\frac{\sum_{i=1}^nX_i - n\mu}{\sigma\sqrt{n}} \leq x\right) = \Phi(x)
	\]
	\label{thm:central-limit}
\end{theorem}
In other words, a sequence of random variables converges in distribution to a normal distribution with variance $\sigma^2$ and mean $\mu$.

These notions of convergence are not identical, and they do not necessarily imply each other.
It is true that almost sure convergence implies convergence in probability, and convergence in probability implies convergence in distribution, but the implication is only one way.


\end{document}

