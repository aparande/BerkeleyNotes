\input{../../header.tex}
\begin{document}
\title{EECS126 Course Notes}
\author{Anmol Parande}
\date{Spring 2021 - Professor Thomas Courtade}
\maketitle
\textbf{Disclaimer: }These notes reflect 126 when I took the course (Spring 2021). They may not accurately reflect current course content, so use at your own risk.
If you find any typos, errors, etc, please raise an issue on the \href{https://github.com/parandea17/BerkeleyNotes}{GitHub repository}.
\tableofcontents
\newpage
\section{Introduction to Probability}
\begin{definition}
  A probability space is a triple $(\Omega, \mathcal{F}, P)$ where $\Omega$ is a set of objects called the sample space, $\mathcal{F}$ is a family of subsets of $\Omega$ called events, and the probability measure $P:\mathcal{F}\rightarrow [0,1]$.
  \label{defn:prob-space}
\end{definition}
One key assumption we make is that $\mathcal{F}$ is a $\sigma$-algebra containing $\Omega$, meaning that countably many complements, unions, and intersections of events in $\mathcal{F}$ are also events in $\mathcal{F}$.
The probability measure $P$ must obey \textbf{Kolmogorov's Axioms}.
\begin{enumerate}
  \item $\forall A \in \mathcal{F},\ P(A) \geq 0$
  \item $P(\Omega) = 1$
  \item If $A_1, A_2, \cdots\in \mathcal{F}$ and $\forall i\ne j,\ A_i\bigcap A_j=\emptyset$, then $P\left(\bigcup_{i\geq 1}A_i\right) = \sum_{i\geq1}P(A_i)$
\end{enumerate}
We choose $\Omega$ and $\mathcal{F}$ to model problems in a way that makes our calculations easy.
\begin{theorem}
  \[
	P(A^c) = 1 - P(A)
  \]
  \label{thm:complementarity}
\end{theorem}
\begin{theorem}[Inclusion-Exclusion Principle]
  \[
	P\left( \bigcup_{i=1}^{n}A_i \right) = \sum_{k=1}^{n}(-1)^{k+1}\left( \sum_{1\leq i_1<\cdots<i_k\leq n} P(A_{i_1}\cap \cdots \cap A_{i_k}) \right)
  \]
  \label{thm:inclusion-exclusion}
\end{theorem}
\begin{theorem}[Law of Total Probability]
  If $A_1, A_2, \cdots$ partition $\Omega$ (i.e $A_i$ are disjoint and $\cup A_i = \Omega$), then for event $B$,
  \[
	P(B) = \sum_iP(B\cap A_i)
  \]
  \label{thm:total-prob}
\end{theorem}
\subsection{Conditional Probability}
\begin{definition}
  If $B$ is an event with $P(B)>0$, then the conditional probability of $A$ given $B$ is
  \[
	P(A|B) = \frac{P(A\cap B)}{P(B)}
  \]
  \label{defn:conditional-prob}
\end{definition}
Intuitively, conditional probabilty is the probability of event $A$ given that event $B$ has occurred.
In terms of probability spaces, it is as if we have taken $(\Omega, \mathcal{F}, P)$ and now have a probabilty measure $P(\cdot|C)$ belonging to the space $(\Omega, \mathcal{F}, P(\cdot|C))$.
\begin{theorem}[Bayes Theorem]
  \[
	P(A|B) = \frac{P(B|A)P(A)}{P(B)}
  \]
  \label{thm:bayes}
\end{theorem}
\subsection{Independence}
\begin{definition}
  Events $A$ and $B$ are independent if $P(A\cap B) = P(A)P(B)$
  \label{defn:independence}
\end{definition}
If $P(B)>0$, then $A, B$ are independent if and only if $P(A|B) = P(A)$. In other words, knowing $B$ occurred gave no extra information about $A$.
\begin{definition}
  If $A,B,C$ with $P(C)>0$ satisfy $P(A\cap B|C) = P(A|C)P(B|C)$, then $A$ and $B$ are conditionally independent given $C$.
  \label{defn:conditional-independence}
\end{definition}
Conditional independence is a special case of independence where $A$ and $B$ are not necessarily independent in the original probability space which has the measure $P$, but are independent in the new probability space conditioned on $C$ with the measure $P(\cdot|C)$.
\section{Random Variables and their Distributions}
\begin{definition}
  A random variable is a function $X:\Omega\rightarrow\mathbb{R}$ with the property $\forall \alpha\in\mathbb{R},\ \{\omega\in\Omega:\ X(\omega) \leq \alpha\} \in \mathcal{F}$.
  \label{defn:rv}
\end{definition}
The condition in \cref{defn:rv} is necessary to compute $P(X\leq \alpha),\ \forall \alpha\in\mathbb{R}$.
This requirement also let us compute $P(X\in B)$ for most sets by leveraging the fact that $\mathcal{F}$ is closed under complements, unions, and intersections.
For example, we can also compute $P(X > \alpha)$ and $P(\alpha < X \leq \beta)$.
In this sense, the property binds the probability space to the random variable.

\cref{defn:rv} also implies that random variables satisfy particular algebraic properties.
For example, if $X,Y$ are random variables, then so are $X+Y, XY, X^p, \lim_{n\to\infty}X_n$, etc.
\begin{definition}
  A discrete random variable is a random variable whose codomain is countable.
  \label{defn:drv}
\end{definition}
\begin{definition}
  A continuous random variable is a random variable whose codomain is the real numbers.
  \label{defn:crv}
\end{definition}
Although random variables are defined based on a probability space, it is often most natural to model problems without explicitly specifying the probability space.
This works so long as we specify the random variables and their distribution in a ``consistent'' way. 
This is formalized by the so-called \href{https://en.wikipedia.org/wiki/Kolmogorov_extension_theorem}{Kolmogorov Extension Theorem} but can largely be ignored.
\subsection{Distributions}
Roughly speaking, the distribution of a random variable gives an idea of the likelihood that a random variable takes a particular value or set of values.
\begin{definition}
  The probability mass function (or distribution) of a random variable $X$ is the frequency with which $X$ takes on different values.
  \[
	p_X:\mathcal{X} \rightarrow [0, 1] \text{ where } \mathcal{X} = \text{range}(X),\qquad p_X(x) = \pr{X=x}.
  \]
  \label{defn:pmf}
\end{definition}
Note that $\sum_{x\in\mathcal{X}}p_X(x) = 1$ since $\bigcap_{x\in\mathcal{X}}\{w: X(w) = x\} = \Omega$.

Continuous random variables are largely similar to discrete random variables.
One key difference is that instead of being described by a probability ``mass'', they are instead described by a probability ``density''.
\begin{definition}
  The probability density function (distribution) of a continuous random variable describes the density by which a random variable takes a particular value.
  \[
	f_X: \mathbb{R}\to [0, \infty) \text{ where } \int_{-\infty}^{\infty}f_X(x)dx = 1 \text{ and } \pr{X\in B} = \int_B f_X(x)dx
  \]
  \label{defn:pdf}
\end{definition}
Observe that if a random variable $X$ is continuous, then the probability that it takes on a particular value is zero.
\[
  \pr{X=x} = \lim_{\delta\to0} \pr{x \leq X \leq x +\delta} = \lim_{\delta\to 0}\int_x^{x+\delta}f_X(u)du = \int_{x}^{x}f_X(u)du = 0
\]
\begin{definition}
  The cumulative distribution function (CDF) gives us the probability of a random variable $X$ being less than or equal to a particular value.
  \[
	F_X:\mathbb{R} \to [0, 1],\quad F_X(x) = \pr{X \leq x}
  \]
  \label{defn:cdf}
\end{definition}
Note that by the Kolomogorov axioms, $F_X$ must satisfy three properties:
\begin{enumerate}
  \item $F_X$ is non-decreasing.
  \item $\lim_{x\to0} F_X(x) = 0$ and $\lim_{x\to\infty} F_X(x) = 1$.
  \item $F_X$ is right continuous.
\end{enumerate}
It turns out that if we have any function $F_X$ that satisfies these three properties, then it is the CDF of some random variable on some probability space.
Note that $F_X(x)$ gives us an alternative way to define continuous random variables.
If $F_X(x)$ is absolutely continuous, then it can be expressed as
\[
  F_X(x) = \int_{-\infty}^{x}f_X(x)dx
\]
for some non-negative function $f_X(x)$, and this is the PDF of a continuous random variable.

Often, when modeling problems, there are multiple random variables that we want to keep track of.
\begin{definition}
  If $X$ and $Y$ are random variables on a common probability space $(\Omega, \mathcal{F}, P)$, then the joint distribution (denoted $p_XY(x, y)$ or $f_{XY}(x, y)$ describes the frequencies of joint outcomes.
  \label{defn:joint-dist}
\end{definition}
Note that it is possible for $X$ to be continuous and $Y$ to be discrete (or vice versa).
\begin{definition}
  The marginal distribution of a joint distribution is the distribution of a single random variable.
  \[
	p_X(x) = \sum_yp_{XY}(x, Y=y), \qquad f_X(x) = \int_{-\infty}^{\infty}f_{XY}(x, y)dy
  \]
  \label{defn:marginal-dist}
\end{definition}
\begin{definition}
  Two random variables $X$ and $Y$ are independent if their joint distribution is the product of the marginal distributions.
  \label{defn:rv-independence}
\end{definition}
Just like independence, we can extend the notion of conditional probability to random variables.
\begin{definition}
  The conditional distribution of $X$ given $Y$ captures the frequencies of $X$ given we know the value of $Y$.
  \[
	p_{X|Y}(x|y) = \frac{P_{XY}(x, y)}{p_Y(y)}, \qquad f_{X|Y}(x|y) = \frac{f_{XY}(x, y)}{f_Y(y)}
  \]
  \label{defn:conditional-dist}
\end{definition}
Often, we need to combine or transform several random variables.
A derived distribution is the obtained by arithmetic of several random variables or applying a function to several (or many) random variables.
Since the CDF of a distribution essentially defines that random variable, it can often be easiest to work backwards from the CDF to the PDF or PMF.
In the special case where we want to find $Y=g(X)$ for a function $g$.
\[
  F_y(y) =\pr{Y \leq y} = \pr{g(x) \leq y} = \pr{X \in g^{-1}([-\infty, y])}, \quad g^{-1}(y) = \{ x: g(x) = y \}.
\]
Another special case of a derived distribution is when adding random variables together.
\begin{theorem}
  The resulting distribution of a sum of two independent random variables is the convolution of the distributions of the two random variables.
  \[
	p_{X+Y}(z) = \sum_{k=-\infty}^{\infty}p_X(k)p_Y(z-k), \quad f_{X+Y}(z) = \int_{-\infty}^{\infty}f_X(x)f_Y(z - x)dx
  \]
  \label{thm:convolution}
\end{theorem}
\subsection{Properties of Distributions}
\subsubsection{Expectation}
\begin{definition}
  The expectation of a random variable describes the center of a distribution,
  \[
	\expect{X}=\sum_{x\in\mathcal{X}}xp_X(x), \quad \expect{X} = \int_{-\infty}^{\infty}xf_X(x)dx
  \]
  provided the sum or integral converges.
  \label{defn:drv-expectation}
\end{definition}
Expectation has several useful properties.
If we want to compute the expectation of a function of a random variable, then we can use the law of the unconscious statisitician.
\begin{theorem}[Law of the Unconscious Statistician]
  \[
	\expect{g(X)} = \sum_{x\in\mathcal{X}}g(x)p_X(x), \quad \expect{g(X)} = \int_{-\infty}^{\infty}g(x)f_X(x)dx
  \]
  \label{thm:unconscious-statistician}
\end{theorem}
Another useful property is its linearity.
\[
  \expect{aX+bY} = a\expect{X}+b\expect{Y},\ \forall a, b\in\mathbb{R}.
\]

Sometimes it can be difficult to compute expectations directly. For disrete distributions, we can use the tail-sum formula.
\begin{theorem}[Tail Sum]
  For a non-negative integer random variable,
  \[
	\expect{X} = \sum_{k=1}^{\infty}\pr{X\geq k}.
  \]
  \label{thm:tail-sum}
\end{theorem}
When two random variables are independent, expectation has some additional properties.
\begin{theorem}
  If $X$ and $Y$ are independent, then
  \[
	\expect{XY} = \expect{X}\expect{Y}.
  \]
  \label{thm:indep-expect}
\end{theorem}
We can apply expectations to conditional distributions as well.
\begin{definition}
  The conditional expectation of a conditional distribution is given by
  \[
	\expect{X|Y=y} = \sum_{x\in\mathcal{X}}xp_{X|Y}(x|y), \quad \expect{X|Y=y} = \int_{-\infty}^{\infty}xf_{X|Y}(x, y)dx
  \]
  \label{defn:drv-conditional-expect}
\end{definition}
Notice that $\expect{X|Y}$ is a function of the random variable $Y$, meaning we can apply \cref{thm:unconscious-statistician}.
\begin{theorem}[Tower Property]
  For all functions $f$,
  \[
	\expect{f(Y)X} = \expect{f(Y)\expect{X|Y}}
  \]
  \label{thm:tower-property}
\end{theorem}
If we apply \cref{thm:tower-property} to the function $f(Y) = 1$, then we can see that $\expect{\expect{X|Y}} = \expect{X}$.

Earlier, we saw that we find a derived distribution by transforming and combining random variables.
Sometimes, we don't need to actually compute the distribution, but only some of its properties.
\begin{definition}
  The nth moment of a random variable is $\expect{X^n}$.
  \label{defn:moment}
\end{definition}
It turns out that we can encode the moments of a distribution into the coefficients of a special power series.
\begin{definition}
  The moment generating function of a random variable $X$ is given by $M_X(t) = \expect{e^{tX}}$.
  \label{defn:mgf}
\end{definition}
Notice that if we apply the power series expansion of $e^{tX}$, we see that \[
  M_X(t) = \sum_{n=0}^{\infty}\frac{t!}{n!}\expect{X^n}.
\]
Thus the nth moment is encoded in the coefficients of the power series and we can retrieve them by taking a derivative:
\[
  \expect{X^n} = \diff[n]{}{t}M_X(t).
\]
Another interesting point to notice is that for a continuous random variable
\[
  M_X(t) = \int_{-\infty}^{\infty}f_X(x)e^{tx}dx
\]
is the Laplace transform of the distribution over the real line,
and for a discrete random variable,
\[
  M_X(t) = \sum_{x=-\infty}^{\infty}p_X(x)e^{tx}
\]
is the Z-transform of the distribution evaluated along the curve at $e^{-t}$.
\begin{theorem}
  If the MGF of a function exists, then it uniquely determines the distribution.
  \label{thm:mgf}
\end{theorem}
This provides another way to compute the distribution for a sum of random variables because we can just multiply their MGF.
\subsubsection{Variance}
\begin{definition}
  The variance of a discrete random variable $X$ describes its spread around the expectation and is given by
  \[
	\var{X} = \expect{(X-\expect{X})^2} = \expect{X^2}-\expect{X}^2.
  \]
  \label{defn:drv-variance}
\end{definition}
\begin{theorem}
  When two random variables $X$ and $Y$ are independent, then 
  \[
	\var{X+Y} = \var{X} + \var{Y}.
  \]
  \label{defn:indep-var}
\end{theorem}
\begin{definition}
  The covariance of two random variables describes how much they depend on each other and is given by
  \[
	\cov{X, Y} = \expect{(X-\expect{X})(Y-\expect{Y})} = \expect{XY} - \expect{X}\expect{Y}.
  \]
  \label{defn:cov}
\end{definition}
If $\cov{X,Y} = 0$ then $X$ and $Y$ are uncorrelated.
\begin{definition}
  The correlation coefficient gives a single number which describes how random variables are correlated.
  \[
	\rho(X, Y) = \frac{\cov{X, Y}}{\sqrt{\var{X}}\sqrt{\var{Y}}}.
  \]
  \label{defn:correlation}
\end{definition}
Note that $-1\leq \rho \leq 1$.
Just as expectation can change when we know additional information, so can variance.
\begin{definition}
  Conditional Variance is the variance of $X$ given the value of $Y$.
  \[
	\var{X|Y=y} = \expect{(X - \expect{X|Y=y})^2 | Y=y} = \expect{X^2|Y=y} - \expect{X|Y=y}^2
  \]
  \label{defn:conditional-var}
\end{definition}
Conditional variance is a random variable just as expectation is.
\begin{theorem}[Law of Total Variance]
  \[
	\var{X} = \expect{\var{X|Y}} + \var{\expect{X|Y}}
  \]
  \label{thm:total-variance}
\end{theorem}
The second term in the law of total variance ($\var{\expect{X|Y}}$) can be interpreted as on average, how much uncertainty there is in $X$ given we know $Y$.
\subsection{Common Distributions}
\subsubsection{Discrete Distributions}
\begin{definition}
  $X$ is uniformly distributed when each value of $X$ has equal probability.
  \[
	X\sim \text{Uniform}(\{ 1, 2, \cdots, n \}) \implies p_X(x)  = \begin{cases}
	  \frac{1}{n} & x = 1, 2, \cdots, n,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:drv-uniform}
\end{definition}
\begin{definition}
  $X$ is a Bernoulli random variable if it is either $0$ or $1$ with $p_X(1) = p$.
  \[
	X\sim\text{Bernoulli}(p) \implies p_X(x) = \begin{cases}
	  1 - p & x=0,\\
	  p & x=1,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = p \qquad \var{X} = (1-p)p
  \]
  \label{defn:bernoulli}
\end{definition}
Bernoulli random variables are good for modeling things like a coin flip where there is a probability of success.
Bernoulli random variables are frequently used as indicator random variables $\mathbbm{1}_A$ where
\[
  \mathbbm{1}_A = \begin{cases}
	1 & \text{if A occurs,}\\
	0 & \text{ else.}
  \end{cases}
\]
When paired with the linearity of expectation, this can be a powerful method of computing the expectation of something.
\begin{definition}
  $X$ is a Binomial random variable when
  \[
	X \sim \text{Binomial}(n, p) \implies p_X(x) = \begin{cases}
	\binom{n}{x} p^x (1-p)^{n-x} & x=0, 1, \cdots, n\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = np \qquad \var{X} = np(1-p)
  \]
  \label{defn:binomial}
\end{definition}
A binomial random variable can be thought of as the number of successes in $n$ trials. 
In other words, 
\[
  X \sim \text{Binomial}(n, p) \implies X = \sum_{i=1}^{n}X_i, \quad X_i \sim \text{Bernoulli}(p).
\]
By construction, if $X\sim\text{Binomial}(n, p)$ and $Y\sim\text{Binomial}(m, p)$ are independent, then $X+Y \sim \text{Binomial}(m+n, p)$.
\begin{definition}
  A Geometric random variable is distributed as
  \[
	X\sim\text{Geom}(p) \implies p_X(x) = \begin{cases}
	  p(1-p)^{x-1} & x=1, 2, \cdots\\
	  0 & \text{ else}.
	\end{cases}
  \]
  \[
	\expect{X} = \frac{1}{p} \qquad \var{X} = \frac{1-p}{p^2}
  \]
  \label{defn:geometric}
\end{definition}
Geometric random variables are useful for modeling the number of trials required before the first success.
In other words,
\[
  X \sim \text{Geom}(p) \implies X = \min\{k \geq 1: X_k=1 \} \text{ where } X_i\sim \text{Bernoulli}(p).
\]
A useful property of geometric random variables is that they are memoryless:
\[
  \pr{X=K+M|X>k} = \pr{X=M}.
\]
\begin{definition}
  A Poisson random variable is distributed as
  \[
	X\sim Poisson(\lambda) \implies p_X(x) = \begin{cases}
	  \frac{\lambda^xe^{-\lambda}}{x!} & x=0, 1, \cdots \\
	  0 & \text{ else.}
	\end{cases}
  \]
  \[
	\expect{X} = \lambda
  \]
  \label{defn:poisson}
\end{definition}
Poisson random variables are good for modeling the number of arrivals in a given interval.
Suppose you take a given time interval and divide it into $n$ chunks where the probability of arrival in chunk $i$ is $X_i \sim \text{Bernoulli}(p_n)$.
Then the total number of arrivals $X_n = \sum_{i=1}^{n}X_i$ is distributed as a Binomial random variable with expectation $np_n=\lambda$.
As we increase $n$ to infinity but keep $\lambda$ fixed, we arrive at the poisson distribution.

A useful fact about Poisson random variables is that if $X\sim\text{Poisson}(\lambda)$ and $Y\sim\text{Poisson}(\mu)$ are independent, then $X+Y \sim \text{Poisson}(\lambda + \mu)$.
\subsubsection{Continuous Distributions}
\begin{definition}
  A continuous random variable is uniformly distributed when the pdf of $X$ is constant over a range.
  \[
	X \sim \text{Uniform}(a, b) \implies f_X(x) = \begin{cases}
	  \frac{1}{b-a} & a \leq x \leq b,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:crv-uniform}
\end{definition}
The CDF of a uniform distribution is given by
\[
  F_X(x) = \begin{cases}
	0, &  x < a,\\
	\frac{x-a}{b-a}, & x\in[a, b)\\
	1, & x \geq b.
  \end{cases}
\]
\begin{definition}
  A continuous random variable is exponentially distributed when its pdf is given by
  \[
	X \sim \text{Exp}(\lambda) \implies f_X(x) = \begin{cases}
	  \lambda e^{-\lambda x} & x \geq 0,\\
	  0 & \text{ else.}
	\end{cases}
  \]
  \label{defn:crv-exp}
\end{definition}
Exponential random variables are the only continuous random variable to have the memoryless property:
\[
  \pr{X > t+s | X > s} = \pr{X > t}, \quad t \geq 0.
\]
The CDF of the exponential distribution is given by
\[
  F_X(x) = \lambda \int_0^{x}e^{-\lambda u}du = 1 - e^{-\lambda x}
\]
\begin{definition}
  $X$ is a Gaussian Random Variable with mean $\mu$ and variance $\sigma^2$ (denoted $X\sim \mathcal{N}(\mu, \sigma^2)$) if it has the PDF
  \[
	f_X(x) = \frac{1}{\sqrt{2\pi\sigma^2}}e^{\frac{-(x-\mu)^2}{2\sigma^2}}
  \]
  \label{defn:normal-rv}
\end{definition}
The standard normal is $X\sim\mathcal{N}(0, 1)$, and it has the CDF
\[
  \Phi(x) = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{\frac{-u^2}{2}} du
\]
There is no closed from for $\Phi(x)$.
It turns out that every normal random variable can be transformed into the standard normal (i.e $\frac{X - \mu}{\sigma} \sim \mathcal{N}(0, 1)$).
Some facts about Gaussian random variables are
\begin{enumerate}
  \item If $X\sim\mathcal{N}(\mu_x, \sigma_x^2),\ Y\sim\mathcal{N}(\mu_y, \sigma_y^2)$ are independent, then $X+Y \sim \mathcal{N}(\mu_x+\mu_y, \sigma_x^2 + \sigma_y^2)$.
  \item If $X,Y$ are independent and $(X+Y), (X-Y)$ are independent, then both $X$ and $Y$ are Gaussian with the same variance.
\end{enumerate}
\section{Concentration}
In real life, for the most part, we can't compute probabilities in closed form.
Instead, we either bound them, or we want to show that $P(A) \approx 0$ or $P(A) \approx 1$.
\subsection{Concentration Inequalities}
\begin{theorem}[Markov's Inequality]
  For a non-negative random variable $X$,
  \[
	\pr{X \geq t} \leq \frac{\expect{X}}{t}, \quad t \geq 0.
  \]
  \label{thm:markov}
\end{theorem}
\begin{theorem}[Chebyshev's Inequality]
  If $X$ is a random variable, then
  \[
	\pr{|X - \expect{X}| \geq t} \leq \frac{\var{X}}{t^2}.
  \]
  \label{thm:chebyshev}
\end{theorem}
Intuitively, \cref{thm:chebyshev} gives gives a ``better'' bound than \cref{thm:markov} because it incorporates the variance of the random variable.
Using this idea, we can define an even better bound that incorporates information from all moments of the random variable.
\begin{definition}[Chernoff Bound]
	For a random variable $X$ and $a\in\mathbb{R}$,\[
		\pr{X \geq a} \leq \frac{\expect{e^{tX}}}{e^{ta}} = e^{-ta}M_x(t).
	\]
	\label{defn:chernoff}
\end{definition}
After computing the Chernoff bound for a general $t$, we can then optimize over it to compute the best bound possible.
\subsection{Convergence}
The idea of convergence brings the mathematical language of limits into probability.
The fundamental question we want to answer is given random variables $X_1, X_2, \cdots$, what does it mean to compute
\[
	\lim_{n\to\infty}X_n.
\]
This question is not as straightforward as it seems because random variables are functions, and there are many ways to define the convergence of functions.
\begin{definition}
	A sequence of random variables converges almost surely to $X$ if \[
		P\left(\lim_{n\to \infty}X_n = X\right) = 1
	\]
	\label{defn:almost-sure}
\end{definition}
One result of almost sure convergence deals with deviations around the mean of many samples.
\begin{theorem}[Strong Law of Large Numbers]
	If $X_1, X_2, \cdots, X_n$ are independently and identically distributed to $X$ where $\expect{X} < \infty$, then $\frac{1}{n}\sum_i X_i$ converges almost surely to $\expect{X}$.
	\label{thm:strong-law}
\end{theorem}
The strong law tells us that for any observed realization, there is a point after which there are no deviations from the mean.
\begin{definition}
	A sequence of random variables converges in probability if \[
		\forall \epsilon > 0, \quad \lim_{n\to\infty}P(|X_n - X| > \epsilon) = 0
	\]
	\label{defn:converge-prob}
\end{definition}
Convergence in probability can help us formalize the intuition that we have which says probability is the frequency with which an even happens over many trials of an event.
\begin{theorem}[Weak Law of Large Numbers]
  Let $X_1, X_2, \cdots, X_n$ be independently and identically distributed according to $X$, and let $M_n = \frac{1}{n}\sum X_i$.
  Then for $\epsilon > 0$, \[
	\lim_{n\to\infty} \pr{|M_n - \expect{X}| > \epsilon} = 0.
  \]
  \label{thm:weak-law}
\end{theorem}
It tells us that the probability of a deviation of $\epsilon$ from the true mean will go to 0 in the limit, but we can still observe these deviations.
Nevertheless, the weak law helps us formalize our intuition about probability.
If $X_1, X_2, \cdots, X_n$ are independently and identically distributed according to $X$, then we can define the empirical frequency
\[
  F_n = \frac{\sum\mathbbm{1}_{X_i\in B}}{n} \implies \expect{F_n} = P(X \in B).
\]
By \cref{thm:weak-law}, \[
  \lim_{n\to\infty}\pr{|F_n - P(X\in B)| > \epsilon} = 0,
\]
meaning over many trials, the empirical frequency is equal to the probility of the event, matching intuition.
\begin{definition}
	A sequence of random variables converges in distribution if \[
		\lim_{n\to\infty}F_{X_n}(x) = F_x(x).
	\]
	\label{defn:converge-dist}
\end{definition}
An example of convergence in distribution is the central limit theorem.
\begin{theorem}[Central Limit Theorem]
	If $X_1, X_2, \cdots$ are independently and identically distributed according to $X$ with $\var{X} = \sigma^2$ and $\expect{X} = \mu$, then
	\[
		\lim_{n\to\infty}P\left(\frac{\sum_{i=1}^nX_i - n\mu}{\sigma\sqrt{n}} \leq x\right) = \Phi(x)
	\]
	\label{thm:central-limit}
\end{theorem}
In other words, a sequence of random variables converges in distribution to a normal distribution with variance $\sigma^2$ and mean $\mu$.

These notions of convergence are not identical, and they do not necessarily imply each other.
It is true that almost sure convergence implies convergence in probability, and convergence in probability implies convergence in distribution, but the implication is only one way.
\section{Information Theory}
Information Theory is a field which addresses two questions
\begin{enumerate}
	\item \textbf{Source Coding:} How many bits do I need to losslessly represent an observation.
	\item \textbf{Channel Coding:} How reliably and quickly can I communicate a message over a noisy channel.
\end{enumerate}
\subsection{Quantifying Information}
Intuitively, for a PMF of a disrete random variable, the surprise associated with a particular realization is $-\log p_X(x)$ since less probable realizations are more surprising.
With this intuition, we can try and quantify the ``expected surprise'' of a distribution.
\begin{definition}
	For a Discrete Random Variablle $X\sim p_X$, the Entropy of $X$ is given by \[
		H(x) = \expect{-\log_2 p_X(x)} = -\sum_{x\in\mathcal{X}} p_X(x)\log_2p_X(x).
	\]
	\label{defn:entropy}
\end{definition}
Alternative interpretations of entropy are the average uncertainty and how random $X$ is.
Just like probabilites, we can define both joint and conditional entropies.
\begin{definition}
	For Discrete Random Variables $X$ and $Y$, the joint entropy is given by \[
		H(X,Y) = \expect{-\log_2p_{XY}(x, y)} = -\sum_{x,y\in\mathcal{X}\times\mathcal{Y}}p_{XY}(x, y)\log_2p_{XY}(x, y).
	\]
	\label{defn:joint-entropy}
\end{definition}
\begin{definition}
	For Discrete Random Variable $X$ and $Y$, the conditional entropy is given by \[
		H(Y|X) = \expect{-\log_2p_{Y|X}(y|x)} = -\sum_{x\in\mathcal{X}}p_X(x)H(Y|X=x).
	\]
	\label{defn:conditional-entropy}
\end{definition}
Conditional entropy has a natural interpretation which is that it tells us how surprised we are to see $Y=y$ given that we know $X=x$.
If $X$ and $Y$ are independent, then $H(Y) = H(Y|X)$ because realizing $X$ gives no additional information about $Y$.
\begin{theorem}[Chain Rule of Entropy]
	\[
		H(X, Y) = H(X) + H(X|Y).
	\]
	\label{thm:chain-entropy}
\end{theorem}
In addition to knowing how much our surprise changes for a random variable when we observe a different random variable, we can also quantify how much additional information observing a random variable gives us about another.
\begin{definition}
	For random variables $X$ and $Y$, the mutual information is given by \[
		I(X;Y) = H(X) - H(X|Y) = H(Y) - H(Y|X).
	\]
	\label{defn:mutual-info}
\end{definition}
\subsection{Source Coding}
Source coding deals with finding the minimal number of bits required to represent data.
This is essentially the idea of lossless compression.
In this case, our message is the sequence of realizations of independently and identically distributed random variables $\left(X_i\right)_{i=1}^n \sim p_X$.
The probability of observing a particular sequence is then \[
	P(x_1, x_2, \cdots, x_n) = \prod_{i=1}^np_X(x_i).
\]
\begin{theorem}[Asymptotic Equipartition Property]
	If we have a sequence of independently and identically distributed random variables $\left(X_i\right)_{i=1}^n \sim p_X$, then $-\frac{1}{n}\log P(x_1, x_2, \cdots, x_n)$ converges to $H(X)$ in probability.
	\label{thm:aep}
\end{theorem}
\cref{thm:aep} tells us that with overwhelming probability, we will observe a sequence that is assigned probability $2^{-nH(X)}$.
Using this idea, we can define a subset of possible observed sequences that in the limit, our observed sequence must belong to with overwhelming probability.
\begin{definition}
	For a fixed $\epsilon > 0$, for each $n\geq 1$, the typical set is given by \[
		A_\epsilon^{(n)} = \left\{ (x_1, x_2, \cdots, x_n) : P(x_1, x_2, \cdots, x_n) \geq 2^{-n(H(X)+\epsilon)} \right\}.
	\]
	\label{defn:typical-set}
\end{definition}
Two important properties of the typical set are that
\begin{enumerate}
	\item $\lim_{n\to\infty}P\left((x_1, x_2, \ldots, x_n) \in A_{\epsilon}^{(n)}\right) = 1$
	\item $|A_{\epsilon}^{(n)}| \geq 2^{n(H(X)+\epsilon)}$
\end{enumerate}
The typical set gives us an easy way to do source coding.
If I have $N$ total objects, then I only need $\log N$ bits to represent each object, so I can define a simple protocol which is
\begin{enumerate}
	\item If $(x_i)_{i=1}^{n} \in A^{(n)}_{\frac{\epsilon}{2}}$, then describe them using the $\log|A^{(n)}_{\frac{\epsilon}{2}}| \leq n\left(H(X)+\frac{\epsilon}{2}\right)$ bits
	\item If $(x_i)_{i=1}^n \not \in A^{(n)}_{\frac{\epsilon}{2}}$, then describe them naiively with $n\log|\mathcal{X}|$ bits.
\end{enumerate}
This makes the average number of bits required to describe a message 
\begin{align*}
	\expect{\text{\# of Bits}} &\leq n(H(X)+\frac{\epsilon}{2})P((x_i)_{i=1}^n\in A_{\frac{\epsilon}{2}}^{(n)}) + n\log |\mathcal{X}|P((x_i)_{i=1}^n\in A_{\frac{\epsilon}{2}}^{(n)}) \\
	&\leq n(H(X)+\frac{\epsilon}{2}) + n \frac{\epsilon}{2} \leq n(H(X)+\epsilon)
\end{align*}
This is the first half of a central result of source coding.
\begin{theorem}[Source Coding Theorem]
	If $(X_i)_{i=1}^n \sim p_X$ are a sequence of independently and identically distributed random varible, then for any $\epsilon > 0$ and $n$ sufficiently large, we can represent $(X_i)_{i=1}^n$ using fewer than $n(H(X) + \epsilon)$ bits. Conversely, we can not losslessly represent $(X_i)_{i=1}^n$ using fewer than $nH(X)$ bits.
	\label{thm:source-coding}
\end{theorem}
This lends a new interpretation of the entropy $H(X)$: it is the average number of bits required to represent $X$.
\subsection{Channel Coding}
Whereas source coding deals with encoding information, channel coding deals with transmitting it over a noisy channel.
In general, we have a message $M$, and encoder, a channel, and a decoder as in \cref{fig:channel-coding}.
\begin{gitbook-image}
  \begin{figure}[H]
	\centering
	\begin{tikzpicture}[auto, node distance=2cm,>=latex']
	  % We start by placing the blocks
	  \node [input, name=input] (input) {};
	  \node [block, name=encoder, right of=input, node distance=2cm] (encoder) {Encoder};
	  \node [block, name=channel, right of=encoder, node distance=5cm] (channel) {Noisy Channel};
	  \node [block, name=Decoder, right of=channel, node distance=5cm] (decoder) {Decoder};
	  \node [output, right of=decoder, node distance=3cm] (output) {};

	  % Once the nodes are placed, connecting them is easy. 
	  \draw [->] (input) -- node {$M$} (encoder);
		\draw [->] (encoder) -- node {$X^{(n)}(M)$} (channel);
		\draw [->] (channel) -- node {$Y^{(n)}$} (decoder);
		\draw [->] (decoder) -- node {$\hat{M}(Y^{(n)})$} (output);	
	\end{tikzpicture}
	\caption{Channel Coding}
	\label{fig:channel-coding}
  \end{figure}
\end{gitbook-image}
Each channel can be described by a conditional probability distribution $p_{Y|X}(y|x)$ for each time the channel is used.
\begin{definition}
	For a channel described by $p_{Y|X}$, the capacity is given by \[
		C = \max_{p_X} I(X; Y).
	\]
	\label{defn:capacity}
\end{definition}
In words, the capacity describes the maximum mutual information between the channel input and output.
\begin{definition}
	Suppose we use the channel $n$ times to send a message that takes on average $H(m)$ bits to encode, then the rate of the channel is \[
		R = \frac{H(M)}{n}
	\]
	\label{defn:rate}
\end{definition}
\begin{theorem}{Channel Coding Theorem}
	For a channel decsribed by $p_{Y|X}$ and $\epsilon>0$ and $R < C$, for all $n$ sufficiently large, there exists a rate $R$ communication scheme that achieves a probability of error less than $\epsilon$. If $R > C$, then the probability of error converges to 1 for any communication scheme. 
	\label{thm:channel-coding}
\end{theorem}
\section{Random Processes}
\begin{definition}
	A random/stochastic process is a sequence of random variables $(X_n)_{n\geq 0}$.
	\label{defn:random-process}
\end{definition}
The random variables in a stochastic process do not have to be independently and identically distributed.
In fact, if they are not, then we can get additional modeling power.
\subsection{Discrete Time Markov Chains}
\begin{definition}
	$(X_n)_{n\geq 0}$ is a Markov Chain if each random variable $X_i$ takes values in a discrete set $S$ (the state space), and, \[
		\forall n \geq 0,\ i,j\in S,\ \pr{X_{n+1}=j|X_n=i,\cdots,X_0=x_0} = \pr{X_{n+1}=i|X_n=j}
	\]
	\label{defn:markov-chain}
\end{definition}
In words, a Markov Chain is a sequence of random variables satisfying the Markov Property where probability of being in a state during the next time step only depends on the current state.
\begin{definition}
	A temporally homogenous Markov Chain is one where the transition probabilities $\pr{X_{n+1}=j|X_n=i} = p_{ij}$ for all $i,j\in S$ and $n\geq 0$.
	\label{defn:temp-homo-markov-chain}
\end{definition}
Temporally Homogenous Markov Chains don't change their transition probabilities over time.
Since the $p_{ij}$ are conditional probabilities, they must satisfy
\begin{enumerate}
	\item $\forall i,j\in S,\ p_{ij} \geq 0$
	\item $\forall i\in S,\ \sum_{j\in S}p_{ij} = 1$
\end{enumerate}
\begin{definition}
	The transition matrix of a Markov Chain is a matrix $P$ where the ijth entry $P_{ij} = p_{ij}$ for all $i,j\in S$.
	\label{defn:transition-mat}
\end{definition}
The transition matrix encodes the one-step transition probabilities of the Markov Chain.
\begin{theorem}[Chapman-Kolmogorov Equation]
	The n-step transition probabilities (i.e starting in $i$ and ending in $j$ $n$ steps later) of the Markov Chain are given by $p_{ij}^{(n)} = P^n_{ij}$.
	\label{thm:chapman-kolmogorov}
\end{theorem}
One useful thing we can comptue with Markov Chain is when the chain first enters a particular state.
\begin{definition}
	For a $A \subset S$, the hitting time of $A$ is given by \[
		T_A = \min_n \{ n\geq 0: X_n\in A\}
	\]
	\label{defn:hitting-time}
\end{definition}
Computing the expected hitting time is an example of a broader type of Markov Chain Analysis called \textbf{First Step Analysis}.
In First Step Analysis, we set up a system of equations that relies on the Markov property to generate a system of equations that only look at the first transition in the chain.
For expected hitting time, these look like
\begin{enumerate}
	\item For $i\not\in A$, $\expect{T_A|X_0 = i} = 1 + \sum_j P_{ij} \expect{T_A|X_0 = j}$
	\item For $i\in A$, $\expect{T_A|X_0 = i} = 0$
\end{enumerate}
\subsubsection{Properties of Markov Chains}
\begin{definition}
	If $\exists n \geq 1$ such that $p_{ij}^{(n)} \ne 0$, then $j$ is accessible from $i$, and we write $i\rightarrow j$.
	\label{defn:accesible}
\end{definition}
\begin{definition}
	States $i$ and $j$ communicate with each other when $i\rightarrow j$ and $j\rightarrow i$. We write this as $i\leftrightarrow j$.
	\label{defn:communicate}
\end{definition}
By convention, we say that $i\leftrightarrow i$.
It turns out that $\leftrightarrow$ is an equivalence relation on the state space $S$.
An equivalence relation means that 
\begin{enumerate}
	\item $\forall i\in S,\ i \leftrightarrow i$
	\item $\forall i,j\in S,\ i\leftrightarrow j \Leftrightarrow j \leftrightarrow i$
	\item $\forall i,j,k \in S, i\leftrightarrow k, k\leftrightarrow j \Rightarrow i \leftrightarrow j$
\end{enumerate}
This means that $\leftrightarrow$ partitions the state-space $S$ into equivalence classes (i.e classes of communicating states).
\begin{definition}
	A Markov Chain is irreducible if $S$ is the only class.
	\label{defn:irreducible-markov}
\end{definition}
\begin{definition}
	An irreducible Markov Chain is reversible if and only if there exists a probability vector $\pi$ that satisfies the \textbf{Detailed Balance Equations}: \[
		\forall i,j \in S,\ \pi_j P_{ij} = \pi_i P_{ji}
	\]
	\label{defn:reversible-markov}
\end{definition}
Markov Chains which satisfy the detailed balance equations are called reversible because if $X_0\sim \pi$, then the random vectors $(X_0, X_1, \cdots, X_n)$ and $(X_n, X_{n-1}, \cdots, X_0)$ are equal in distribution.
\begin{theorem}
	If the graph of a Markov Chain (transform the state transition diagram by making edges undirected, removing self-loops, and removing multiple edges) is a tree, then the Markov Chain is reversible.
	\label{thm:tree-markov}
\end{theorem}
\subsubsection{Class Properties}
A class property is a property where if one element of a class has the property, all elements of the class have the property.
Markov Chains have several of these properties which allow us to classify states.
\begin{definition}
	A state $i\in S$ is recurrent if given that $X_0=i$, the process revisits state $i$ with probability 1.
	\label{defn:markov-recurrence}
\end{definition}
\begin{definition}
	A state is $i\in S$ is transient if it is not recurrent.
	\label{defn:markov-transience}
\end{definition}
Recurrence means that we will visit a state infinitely often in the future if we start in that state, while transience means we will only visit the state finitely many times.
We can further break recurrence down if we modify the definition of hitting time to be $T_i = \min_n \{ n \geq 1 : X_n=i \}$ (the first time the chain enters state $i$).
\begin{definition}
	State $i$ is positive recurrent if it is recurrent and $\expect{T_i|X_0=i}$ is finite.
	\label{defn:positive-recurrence}
\end{definition}
\begin{definition}
	State $i$ is null recurrent if it is recurrent and $\expect{T_i|X_0=i}$ is infinite.
	\label{defn:null-recurrence}
\end{definition}
Positive recurrence means we visit a recurrent state so frequently that we spend a positive fraction of time in that state.
Null recurrencce means we visit a recurrent state so infrequently (but still infinitely many times) that we spend virtually no time in that state.
\begin{theorem}
	Every irreducible finite state Markov Chain is positive recurrent.
	\label{thm:positive-recurrence}
\end{theorem}
\begin{definition}
	For a state $i\in S$, we define the period of the state to be
	\[
		\text{period}(i) = \text{GCD}\{n\geq 1 : p_{ii}^{(n)} > 0 \}.
	\]
	\label{defn:period}
\end{definition}
If we start in state $i$, then revists to $i$ only occur at integer multiples of the period.
\begin{definition}
	An irreducible markov chain is aperiodic if any state has period 1.
	\label{defn:aperiodic-mc}
\end{definition}
All of the above properties are class properties.
\subsubsection{Long-Term Behavior of Markov Chains}
Since the $p_{ij}$ completely characterize the Markov Chain, we can also describe what happens to the chain in the limit.
\begin{definition}
	A probability distribution $\pi$ over the states is a stationary distribution if $\pi = \pi P$
	\label{defn:stationary-dist-mc}
\end{definition}
It is called a stationary distribution because the distribution over states is invariant with time.
The relationship $\pi = \pi P$ can be expanded for the jth element to show that any stationary distribution must satisfy the \textbf{Global Balance Equations}:
\[
	\pi_j = \sum_i p_{ij}\pi_i.
\]
Note that if a distribution $\pi$ satisfies the detailed balance equations from \cref{defn:reversible-markov}, then $\pi$ also satisfies \cref{defn:stationary-dist-mc}.

Both the global balance equations and detailed balance equations can be conceptualized as statements of flow.
If each $\pi_j$ indicates how much mass is placed on state $j$, then the global balance equations tell us the mass leaving the node (going to each neighbor $i$ in proportion to $p_{ij}$) is equal to the mass entering the node (which must sum to $\pi_j$ since it is a stationary distribution.
Rather than looking at the flow of the whole chain, the detailed balance equations look at the flow between two states. The mass $i$ gives to $j$ is equal to the mass $j$ gives to $i$.
\begin{theorem}[Big Theorem for Markov Chains]
	Let $(X_n)_{n\geq 0}$ be an irreducible Markov Chain. Then one of the following is true.

	1. Either all states are transient, or all states are null recurrent, and no stationary distribution exists, and $\lim_{n\to\infty}p_{ij}^{(n)} = 0$.
	% TODO: Something weird is in this statement. See Piazza (https://piazza.com/class/kjow2zzmpvg3af?cid=274)
	
	2. All states are positive recurrent and the stationary distribution exists, is unique, and satisfies \[
		\pi_j = \lim_{n\to\infty}\frac{1}{n}\sum_{k=0}^{n}P_{ij}^{(k)} = \frac{1}{\expect{T_j|X_0=j}}.
		\]
		If the Markov Chain is periodic, then $\lim_{n\to\infty}p_{ij}^{(n)} = \pi_j$
	\label{thm:big-theorem}
\end{theorem}
One consequence of \cref{thm:big-theorem} is that it means the stationary distribution $\pi$ of a reversible Markov Chain is unique.
This makes solving the detailed balance equations a good technique of solving for the stationary distribution.
\subsection{Continuous Time Markov Chains}
\begin{definition}
	A process $(X_t)_{t\geq 0}$ taking values in a countable state space $S$ is a temporally homogenous continuous time markov chain if it satisfies the Markov Property
	\[
		\pr{X_{t+\tau}=j|X_t=i,X_s=i_s, 0 \leq s \leq t} = \pr{X_{t+\tau}=j|X_t=i} = \pr{X_\tau = j | X_0 = i}
	\]
	\label{defn:ctmc}
\end{definition}
To characterize how a CTMC functions, we need to define some additional quantities.
\begin{enumerate}
	\item $q_i$ is the transition rate of state $i$
	\item $p_{ij}$ is the transition probability bewteen states $i$ and $j$
\end{enumerate}
Every time a CTMC enters a state $i$, it will hold in that state for $\text{Exp}(q_i)$ time before transitioning to the next state $j$ with probability $p_{ij}$.
\begin{definition}
	The jump chain is a DTMC which describes the transition probabilities between states in the CTMC
	\label{defn:jump-chain}
\end{definition}
Note that the jump chain cannot have self-loops ($p_{ii}=0$) because otherwise the amount of time spent in state $i$ would not be exponentially distributed.
An alternative interpretation of a CTMC is
\begin{enumerate}[1.]
	\item Define jump rates $q_{ij} = q_i p_{ij}$
	\item On entering state $i$, jump to $j^\star = \argmin_j T_j$ where $T_j \sim \text{Exp}(q_{ij})$ for all $j\neq i$ and are independent from each other.
\end{enumerate}
Essentially, every time we enter a state, we set an alarm clock for all other states, and then jump to the state whose alarm clock goes off first.
This equivalent interpretation allows us to summarize a CTMC using the rate matrix.
\[
	Q_{ij} = \begin{cases}
		-q_i & \text{ if } i=j\\
		q_{ij} & \text{ if } i \neq j
	\end{cases}
\]
Following from the first interprentation, all entries of $Q$ are non-negative, and the rows must sum to 0.
One useful quantity which we can define is how long it takes to come back to a particular state.
\begin{definition}
	The time to first re-entry of state $j$ is \[
		T_j = \min \{t \geq 0: X_t=j \text{ and } X_s \neq j \text{ for some } s < t\}
	\]
	\label{defn:ctmc-reentry}
\end{definition}
\subsubsection{Class Properties}
Just like in DTMCs, we can classify states in the CTMC.
\begin{definition}
	States $i$ and $j$ communicate with eachc other if $i$ and $j$ communicate in the jump chain.
	\label{defn:ctmc-communicate}
\end{definition}
\begin{definition}
	State $j$ is transient if given $X_0=j$, the process enters $j$ finitely many times with probability 1. Otherwise, it is recurrent.
	\label{defn:ctmc-recurrent}
\end{definition}
\begin{definition}
	A state $j$ is positive recurrent if its time to first re-entry is finite, and null recurrent otherwise.
	\label{defn:ctmc-recurrence}
\end{definition}
\subsubsection{Long Term Behavior of CTMCs}
CTMCs also have stationary distributions.
\begin{definition}
	A probability vector $\pi$ is a stationary ditribution for a CTMC with rate matrix $Q$ if \[
		\pi Q = 0 \Leftrightarrow \pi_jq_j = \sum_{i\neq j}\pi_iq_{ij}.
	\]
	\label{defn:ctmc-stationary}
\end{definition}
To describe how a CTMC behaves over time, first define $p_{ij}^{(t)} = \pr{X_t=j|X_0=i}$ and $m_j = \expect{T_j|X_0=j}$.
\begin{theorem}[Big Theorem for CTMCs]
	For an irreducible CTMC, exactly one of the following is true.

	1. All states are transient or null recurrent, no stationary distribution exists, and $\lim_{t\to\infty}p_{ij}^{(t)} = 0$
	
	2. All states are positive recurrent, a unique stationary distribution exists, and the stationary distribution satisfies \[
		\pi_j = \frac{1}{m_jq_j} = \lim_{t\to\infty}p_{ij}^{(t)}
	\]
	\label{thm:ctmc-big}
\end{theorem}
\subsubsection{Poisson Processes}
\begin{definition}
	A counting process $(N_t)_{t\geq 0}$ is a non-decreasing, continuous time, integer valued random process which has right continuous sample paths.
	\label{defn:counting-process}
\end{definition}
There are two important metrics which describe counting processes.
\begin{definition}
	The ith arrival time $T_i$ is given by \[
		T_i = \min_t \{ t \geq 0: \ N_t \geq i \}
	\]
	\label{defn:arrival-time}
\end{definition}
\begin{definition}
	The ith inter-arrival time $S_i$ is given by \[
		S_i = T_i - T_{i-1}, i > 0
	\]
	\label{defn:inter-arrival-time}
\end{definition}
\begin{definition}
	A rate $\lambda$ Poisson Process is a counting process with independently and identically distributed inter-arrival times $S_i \sim \text{Exp}(\lambda)$.
	\label{defn:poisson-process}
\end{definition}
The name Poisson comes from the distribution of each varible in the process.
\begin{theorem}
	If $(N_t)_{t\geq 0}$ is a rate $\lambda$ Poisson Process, then for each $t\geq 0$, $N_t\sim \text{Poisson}(\lambda t)$
	\label{thm:poisson-process}
\end{theorem}
A Poisson Process is a special case of a CTMC where the transition rates $q_i = \lambda$ and the transition probabilties $p_{ij}$ are 1 if $j=i+1$ and 0 otherwise.
Since the inter-arrival times are memoryless and i.i.d, Poisson Processes have many useful properties.
\begin{theorem}
	If $(N_t)_{t\geq 0}$ is a rate $\lambda$ Poisson Process, then $(N_{t+s} - N_s)_{t\geq0}$ is also a rate $\lambda$ Poisson Process for all $s \geq 0$ and is independent of the original process.
	\label{thm:indep-poisson-process}
\end{theorem}
\begin{theorem}
	For $t_0 < t_1 <\ldots< t_k$, then the increments of a rate $\lambda$ Poisson Process $(N_{t_1} - N_{t_0}), (N_{t_2} - N_{t_1}),\ldots,(N_{t_k} - N_{t_{k-1}})$ are independent and $N_{t_i} - N_{t_{i-1}} \sim \text{Poisson}(\lambda(t_i - t_{i-1}))$
	\label{thm:indep-increments}
\end{theorem}
Poisson Processes are the only counting processes with these particular properties.

It turns out that Poisson Processes can be connected with the Order Statistics of Uniform Random Variables.
\begin{theorem}[Conditional Distribution of Arrivals]
	Conditioned on $N_t = n$, the random vector $T_1, T_2, \cdots, T_n$ has the same distribution as the order statistics of $n$ random variables $U\sim \text{Uniform}(0, t)$.
	\label{thm:conditional-dist-of-arrivals}
\end{theorem}
What \cref{thm:conditional-dist-of-arrivals} says is that given $n$ arrivals up to time $t$ occur, the distribution of arrival times is equivalent to taking $n$ i.i.d uniform random variables and sorting them.

Two other useful properties of Poisson Processes involve combining and separating them.
\begin{theorem}[Poisson Merging]
	If $N_{1,t}$ and $N_{2,t}$ are independent Poisson Processes with rates $\lambda_1$ and $\lambda_2$, then $N_{1, t} + N_{2,t}$ is a Poisson Process with rate $\lambda_1+\lambda_2$.
	\label{thm:poisson-merging}
\end{theorem}
\begin{theorem}[Poisson Splitting]
	Let $p(x)$ be a probability distribution and $N_t$ be a rate $\lambda$ Poisson process. If each arrival is marked with the label $i$ independently with probability $p(x=i)$, then $N_{i,t}$, the process counting the number of arrivals labeled $i$ is an independent Poisson Process with rate $\lambda p_i$.
	\label{thm:poisson-splitting}
\end{theorem}
\end{document}

