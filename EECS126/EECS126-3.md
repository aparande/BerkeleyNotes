# Concentration

In real life, for the most part, we can’t compute probabilities in
closed form. Instead, we either bound them, or we want to show that
$$P(A) \approx 0$$ or $$P(A) \approx 1$$.

## Concentration Inequalities

{% hint style="info" %}

### Theorem 17 (Markov's Inequality) {#theorem-17}

For a non-negative random variable $$X$$,


$$ \text{Pr}\left\{X \geq t\right\}  \leq \frac{\mathbb{E}\left[X\right] }{t}, \quad t \geq 0. $$

{% endhint %}

{% hint style="info" %}

### Theorem 18 (Chebyshev's Inequality) {#theorem-18}

If $$X$$ is a random variable, then


$$ \text{Pr}\left\{|X - \mathbb{E}\left[X\right] | \geq t\right\}  \leq \frac{\text{Var}\left(X\right) }{t^2}. $$

{% endhint %}

Intuitively, theorem 18 gives gives a “better” bound than theorem 17
because it incorporates the variance of the random variable. Using this
idea, we can define an even better bound that incorporates information
from all moments of the random variable.

{% hint style="info" %}

### Definition 36

\\pr{X \\geq a} \\leq \\frac{\\expect{e\^{tX}}}{e\^{ta}} = e\^{-ta}M_x(t).
$$$$

{% endhint %}

After computing the Chernoff bound for a general $$t$$, we can then
optimize over it to compute the best bound possible.

## Convergence

The idea of convergence brings the mathematical language of limits into
probability. The fundamental question we want to answer is given random
variables $$X_1, X_2, \cdots$$, what does it mean to compute

$$\lim_{n\to\infty}X_n.$$

This question is not as straightforward as it seems because random
variables are functions, and there are many ways to define the
convergence of functions.

{% hint style="info" %}

### Definition 37

A sequence of random variables converges almost surely to $$X$$ if 

$$ P\left(\lim_{n\to \infty}X_n = X\right) = 1 $$

{% endhint %}

One result of almost sure convergence deals with deviations around the
mean of many samples.

{% hint style="info" %}

### Theorem 19 (Strong Law of Large Numbers) {#theorem-19}

If $$X_1, X_2, \cdots, X_n$$ are independently and identically distributed to $$X$$ where $$\mathbb{E}\left[X\right]  < \infty$$, then $$\frac{1}{n}\sum_i X_i$$ converges almost surely to $$\mathbb{E}\left[X\right] $$.

{% endhint %}

The strong law tells us that for any observed realization, there is a
point after which there are no deviations from the mean.

{% hint style="info" %}

### Definition 38

A sequence of random variables converges in probability if 

$$ \forall \epsilon > 0, \quad \lim_{n\to\infty}P(|X_n - X| > \epsilon) = 0 $$

{% endhint %}

Convergence in probability can help us formalize the intuition that we
have which says probability is the frequency with which an even happens
over many trials of an event.

{% hint style="info" %}

### Theorem 20 (Weak Law of Large Numbers) {#theorem-20}

Let $$X_1, X_2, \cdots, X_n$$ be independently and identically distributed according to $$X$$, and let $$M_n = \frac{1}{n}\sum X_i$$.
Then for $$\epsilon > 0$$, 

$$ \lim_{n\to\infty} \text{Pr}\left\{|M_n - \mathbb{E}\left[X\right] | > \epsilon\right\}  = 0. $$

{% endhint %}

It tells us that the probability of a deviation of $$\epsilon$$ from the
true mean will go to 0 in the limit, but we can still observe these
deviations. Nevertheless, the weak law helps us formalize our intuition
about probability. If $$X_1, X_2, \cdots, X_n$$ are independently and
identically distributed according to $$X$$, then we can define the
empirical frequency

$$F_n = \frac{\sum\mathbb{1}_{X_i\in B}}{n} \implies \mathbb{E}\left[F_n\right]  = P(X \in B).$$

By theorem 20,

$$\lim_{n\to\infty}\text{Pr}\left\{|F_n - P(X\in B)| > \epsilon\right\}  = 0,$$

meaning over many trials, the empirical frequency is equal to the
probility of the event, matching intuition.

{% hint style="info" %}

### Definition 39

A sequence of random variables converges in distribution if 

$$ \lim_{n\to\infty}F_{X_n}(x) = F_x(x). $$

{% endhint %}

An example of convergence in distribution is the central limit theorem.

{% hint style="info" %}

### Theorem 21 (Central Limit Theorem) {#theorem-21}

If $$X_1, X_2, \cdots$$ are independently and identically distributed according to $$X$$ with $$\text{Var}\left(X\right)  = \sigma^2$$ and $$\mathbb{E}\left[X\right]  = \mu$$, then


$$ \lim_{n\to\infty}P\left(\frac{\sum_{i=1}^nX_i - n\mu}{\sigma\sqrt{n}} \leq x\right) = \Phi(x) $$

{% endhint %}

In other words, a sequence of random variables converges in distribution
to a normal distribution with variance $$\sigma^2$$ and mean $$\mu$$.

These notions of convergence are not identical, and they do not
necessarily imply each other. It is true that almost sure convergence
implies convergence in probability, and convergence in probability
implies convergence in distribution, but the implication is only one
way.

Once we know how a random variable converges, we can then also find how
functions of that random variable converge.

{% hint style="info" %}

### Theorem 22 (Continuous Mapping Theorem) {#theorem-22}

If $$f$$ is a continuous function, then if $$X_n$$ converges to $$X$$, then $$f(X_n)$$ converges to $$f(X)$$. The convergence can be almost surely, in probability, or in distribution.

{% endhint %}

