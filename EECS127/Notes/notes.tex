\input{../../header.tex}
\begin{document}
\title{EECS127 Course Notes}
\author{Anmol Parande}
\date{Spring 2021 - Professor Laurent El Ghaoui}
\maketitle
\textbf{Disclaimer: }These notes reflect 127 when I took the course (Spring 2021). They may not accurately reflect current course content, so use at your own risk.
If you find any typos, errors, etc, please raise an issue on the \href{https://github.com/parandea17/BerkeleyNotes}{GitHub repository}.
\tableofcontents
\newpage
\section{Linear Algebra}
\begin{definition}
  An affine set is one of the form $\mathcal{A}=\{ \V{x}\in\mathcal{X}:\ \V{x}=\V{v}+\V{x_0},\ \V{v}\in\mathcal{V}\}$ where $\mathcal{V}$ is a subspace of a vector space $\mathcal{X}$ and $x_0$ is a given point.
  \label{defn:affine-set}
\end{definition}
Notice that by \cref{defn:affine-set}, a subspace is simply an affine set containing the origin. Also notice that the dimension of an affine set $\mathcal{A}$ is the same as the dimension of $\mathcal{V}$.
\subsection{Norms}
\begin{definition}
  A norm on the vector space $\mathcal{X}$ is a function $\|\cdot\|:\mathcal{X}\rightarrow\mathbb{R}$ which satisfies:
	1. $\|\V{x}\|\geq 0$ with equality if and only if $\V{x}=\bs{0}$

	2. $\|\V{x}+\V{y}\|\leq\|\V{x}\|+\|\V{y}\|$

	3. $\|\alpha \V{x}\| = |\alpha|\|\V{x}\|$ for any scalar $\alpha$.
  \label{defn:norm}
\end{definition}
\begin{definition}
  The $l_p$ norms are defined by
  \[
		\|\V{x}\|_p=\left( \sum_{k=1}^n|x_k|^p \right)^{\frac{1}{p}},\ 1\leq p\leq \infty
  \]
  \label{defn:lp-norm}
\end{definition}
In the limit as $p\to\infty$, \[
	\|\V{x}\|_{\infty} = \max_k|x_k|.
\]
Similar to vectors, matrices can also have norms.
\begin{definition}
  A function $f: \mathbb{R}^{m\times n} \to \mathbb{R}$ is a matrix norm if 
  \[
		f(A) \geq 0 \quad f(A) = 0 \Leftrightarrow A = 0 \quad f(\alpha A) = |\alpha| f(A) \quad f(A+B) \leq f(A) + f(B)
  \]
  \label{defn:matrix-norm}
\end{definition}
\begin{definition}
  The Froebenius norm is the $l_2$ norm applied to all elements of the matrix.
  \[
	\|A\|_F = \sqrt{\text{trace} AA^T} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}
  \]
  \label{defn:froebenius-norm}
\end{definition}
One useful way to characterize matrices is by measuring their ``gain'' relative to some $l_p$ norm.
\begin{definition}
  The operator norms is defined as
  \[
	\|A\|_p = \max_{\V{u}\ne0} \frac{\|A\V{u}\|_p}{\|u\|_p}
  \]
  \label{defn:operator-norm}
\end{definition}
When $p=2$, the norm is called the spectral norm because it relates to the largest eigenvalue of $A^TA$.
\[
	\|A\|_2 = \sqrt{\lambda_{max}(A^TA)}
\]
\subsection{Inner Products}
\begin{definition}
  An inner product on real vector space is a function that maps $\V{x},\V{y} \in \mathcal{X}$ to a non-negative scalar, is distributive, is commutative, and $\langle \V{x}, \V{x}, \rangle = 0 \Leftrightarrow \V{x}=0$.
  \label{defn:inner-product}
\end{definition}
Inner products induce a norm $\|\V{x}\| = \sqrt{\langle \V{x}, \V{x} \rangle}$.
In $\mathbb{R}^n$, the standard inner product is $\V{x}^T\V{y}$.
The angle bewteen two vectors is given by
\[
	\cos\theta = \frac{\V{x}^T\V{y}}{\|\V{x}\|_2\|\V{y}\|_2}.
\]
In general, we can bound the absolute value of the standard inner product between two vectors.
\begin{theorem}[Holder Inequality]
  \[
	|\V{x}^T\V{y}| \leq \sum_{k=1}^n |x_ky_k| \leq \|\V{x}\|_p\|\V{y}\|_q,\ p, q\geq 1 \text{ s.t } p^{-1}+q^{-1}=1.
  \]
  \label{thm:holder}
\end{theorem}
Notice that for $p=q = 2$, \cref{thm:holder} turns into the Cauchy-Schwartz Inequality ($|\V{x}^T\V{y}| \leq \|\V{x}\|_2\|\V{y}\|_2$).
\subsection{Functions}
We consider functions to be of the form $f:\mathbb{R}^n\rightarrow\mathbb{R}$.
By contrast, a map is of the form $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$.
The components of the map $f$ are the scalar valued functions $f_i$ that produce each component of a map.
\begin{definition}
  The graph of a function $f$ is the set of input-output pairs that $f$ can attain.
  \[
	\left\{ (x, f(x))\in \mathbb{R}^{n+1}:\ x\in\mathbb{R}^n \right\}
  \]
  \label{defn:graph}
\end{definition}
\begin{definition}
  The epigraph of a function is the set of input-output pairs that $f$ can achieve and anything above.
  \[
	\left\{ (x,t) \in \mathbb{R}^{n+1}:\ \V{x}\in\mathbb{R}^{n+1},\ t\geq f(x) \right\}
  \]
  \label{defn:epigraph}
\end{definition}
\begin{definition}
  The t-level set is the set of points that achieve exactly some value of $f$.
  \[
	\{ \V{x}\in\mathbb{R}^n:\ f(x)=t \}
  \]
  \label{defn:level-set}
\end{definition}
\begin{definition}
  The t-sublevel set of $f$ is the set of points achieving at most a value $t$.
  \[
	\{ x\in\mathbb{R}^n:\ f(x)\leq t \}
  \]
  \label{defn:sublevel-set}
\end{definition}
\begin{definition}
  The half-spaces are the regions of space which a hyper-plane separates.
  \[
	H_{\_} = \{ x: \V{a}^T\V{x}\leq b \} \qquad H_{+} = \{ x: \V{a}^T\V{x} > b \}
  \]
  \label{defn:halfspace}
\end{definition}
\begin{definition}
	A polyhedron is the intersection of $m$ half-spaces given by $\V{a}_i^T\V{x}\leq b_i$ for $i\in[1,m]$.
	\label{defn:polyhedron}
\end{definition}
When a polyhedron is bounded, it is called a polytope.
\subsubsection{Types of Functions}
\begin{theorem}
  A function is linear if and only if it can be expressed as $f(\V{x}) = \V{a}^T\V{x}+b$ for some unique pair $(\V{a}, b)$.
  \label{thm:linear-ip}
\end{theorem}
An affine function is linear when $b=0$. A hyperplane is simply a level set of a linear function.
\begin{theorem}
	Any quadratic function can be written as the sum of a quadratic term involving a symmetric matrix and an affine term:
	\[
		q(x) = \frac{1}{2}\V{x}^TH\V{x}+\V{c}^T\V{x} + d.
	\]
	\label{thm:quadratic}
\end{theorem}
Another special class of functions are polyhedral functions.
\begin{definition}
	A function $f:\mathbb{R}^n\to\mathbb{R}$ is polyhedral if its epigraph is a polyhedron.
	\[
		\text{epi } f = \left\{(x,t) \in \mathbb{R}^{n+1} :\ C \begin{bmatrix}\V{x} \\ t \end{bmatrix} \leq d \right\}
	\]
	\label{defn:polyhedral-function}
\end{definition}
\subsubsection{Vector Calculus}
We can also do calculus with vector functions.
\begin{definition}
  The gradient of a function at a point $x$ where $f$ is differentiable is a column vector of first derivatives of $f$ with respsect to the components of $\V{x}$
  \[
	\nabla f(x) = \begin{bmatrix}
	  \frac{\partial f}{\partial x_1}\\
	  \vdots\\
	  \frac{\partial f}{\partial x_n}
	\end{bmatrix}
  \]
  \label{defn:gradient}
\end{definition}
The gradient is perpendicular to the level sets of $f$ and points from a point $\V{x}_0$ to higher values of the function.
In other words, it is the direction of steepest increase.
It is akin to the derivative of a 1D function.
\begin{definition}
	The Hessian of a function $f$ at point $x$ is a matrix of second derivatives.
	\[
		H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}
	\]
	\label{defn:hessian}
\end{definition}
The Hessian is akin to the second derivative in a 1D function.
Note that the Hessian is a symmetric matrix.
\subsection{Matrices}
Matrices define a linear map between an input space and an output space. 
Any linear map $f: \mathbb{R}^n \to \mathbb{R}^m$ can be represented by a matrix.
\begin{theorem}[Fundamental Theorem of Linear Algebra]
  For any matrix $A\in\mathbb{R}^{m\times n}$,
  \[
	\mathcal{N}(A) \oplus \mathcal{R}(A^T) = \mathbb{R}^n \qquad \mathcal{R}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m.
  \]
  \label{thm:fundamental}
\end{theorem}
\subsubsection{Symmetric Matrices}
Recall that a symmetric matrix is one where $A = A^T$.
\begin{theorem}[Spectral Theorem]
  Any symmetric matrix is orthogonally similar to a real diagonal matrix.
  \[
	A = A^T \implies A = U \Lambda U^T = \sum_i \lambda_i \V{u}_i\V{u}_i^T,\quad \|\V{u}\| = 1, \quad \V{u}_i^T\V{u}_j = 0 \ (i \ne j)
  \]
  \label{thm:spectral}
\end{theorem}
Let $\lambda_{min}(A)$ be the smallest eigenvalue of symmetric matrix $A$ and $\lambda_{max}(A)$ be the largest eigenvalue.
\begin{definition}
	The Rayleigh Quotient for $\V{x} \ne \bs{0}$ is $\frac{\V{x}^TA\V{x}}{\|\V{x}\|^2}.$
  \label{defn:rayleigh-quotient}
\end{definition}
\begin{theorem}
	For any $\V{x} \ne \bs{0}$,
  \[
		\lambda_{min}(A) \leq \frac{\V{x}^TA\V{x}}{\|\V{x}\|^2} \leq \lambda_{max}(A).
  \]
\end{theorem}
Two special types of symmetric matrices are those with non-negative eigenvalues.
\begin{definition}
  A symmetric matrix is positive semi-definite if $\V{x}^TA\V{x} \geq 0 \implies \lambda_{min}(A) \geq 0$.
  \label{defn:psd-matrix}
\end{definition}
\begin{definition}
  A symmetric matrix is poitive definite if $\V{x}^TA\V{x} > 0 \implies \lambda_{min}(A) > 0$.
  \label{defn:pd-matrix}
\end{definition}
These matrices are important because they often have very clear geometric structures.
For example, an ellipsoid in multi-dimensional space can be defined as the set of points
\[
  \mathcal{E} = \{ x\in\mathbb{R}^m : \ \V{x}^T P^{-1} \V{x} \leq 1 \} 
\]
where $P$ is a positive definite matrix.
The eigenvectors of $P$ give the principle axes of this ellipse, and $\sqrt{\lambda}$ are the semi-axis lengths.
\subsubsection{QR Factorization}
Similar to how spectral theorem allows us to decompose symmetric matrices, QR factorization is another matrix decomposition technique that works for any general matrix.
\begin{definition}
  The QR factorization matrix are the orthogonal matrix Q and the upper triangular matrix R such that $A = QR$
  \label{defn:qr-factorization}
\end{definition}
An easy way to find the QR factorization of a matrix is to apply Graham Schmidt to the columns of the matrix and express the result in matrix form.
Suppose that our matrix $A$ is full rank (i.e its columns $\V{a}_i$ are linearly independent) and we have applied Graham-Schmidt to columns $\V{a}_{i+1}\cdots\V{a}_n$ to get orthogonal vectors $\V{q}_{i+1}\cdots\V{q}_{n}$.
Continuing the procedure, the ith orthogonal vector $\V{q}_i$ is
\[
  \V{\tilde{q}}_i = \V{a}_i - \sum_{k=i+1}^{n} (\V{q}_k^T \V{a}_k)\V{q}_k \qquad \V{q}_i = \frac{\V{\tilde{q}}_i}{\|\V{\tilde{q}}_i\|_2}.
\]
If we re-arrange this, to solve for $\V{a}_i$, we see that
\[
  \V{a}_i = \|\V{\tilde{q}}_i\|_2 \V{q}_i + \sum_{k=i+1}^{n} (\V{q}_k^T \V{a}_k)\V{q}_k.
\]
Putting this in matrix form, we can see that
\[
  \begin{bmatrix}
	| & | & & | \\
	\V{a}_1 & \V{a}_2 & \cdots & \V{a}_{n}\\
	| & | & & | \\
  \end{bmatrix} = \begin{bmatrix}
	| & | & & | \\
	\V{q}_1 & \V{q}_2 & \cdots & \V{q}_{n}\\
	| & | & & | \\
  \end{bmatrix} \begin{bmatrix}
	r_{11} & r_{12} & \cdots & r_{1n}\\
	0 & r_{22} & \cdots & r_{2n}\\
	\vdots & \ddots & \ddots & \vdots\\
	0 & \cdots & 0 & r_{nn}
  \end{bmatrix} \qquad r_{ij} = \V{a}_i^T\V{q_j}, r_{ii} = \|\V{\tilde{q}}_i\|_2.
\]

\subsubsection{Singular Value Decomposition}
\begin{definition}
	A matrix $A\in\mathbb{R}^{m\times n}$ is a dyad if it can be written as $\V{p}\V{q}^T$.
	\label{defn:dyad}
\end{definition}
A dyad is a rank-one matrix.
It turns out that all matrices can be decomposed into a sum of dyads.
\begin{definition}
	The Singular Value Decomposition of a matrix $A$ is \[
		A = \sum_{i=1}^{r} \sigma_i \V{u}_i\V{v}_i^T
	\]
	where $\sigma_i$ are the singular values of $A$ and $\V{u}_i$ and $\V{v}_i$ are the left and right singular vectors.
	\label{defn:svd}
\end{definition}
Th singular values are ordered such that $\sigma_1 >= \sigma_2 >= \cdots$.
The left singular values are the eigenvectors of $AA^T$ and the right singular values are the eigenvectors of $A^TA$.
The singular values are $\sqrt{\lambda}_i$ where $\lambda_i$ are the eigenvalues of $A^TA$.
Since $AA^T$ and $A^TA$ are symmetric, $\V{u}_i$ and $\V{v}_i$ are orthogonal.
The number of non-zero singular values is equal to the rank of the matrix.
We can write the SVD in matrix form as \[
	A = \left[U_r\quad U_{n-r}\right]\text{diag}(\sigma_1,\cdots,\sigma_r,0,\cdots,0)\begin{bmatrix}V^T_r\\V^T_{n-r}\end{bmatrix}
\]
Writing the SVD tells us that 
\begin{enumerate}
	\item $V_{n-r}$ forms a basis for $\mathcal{N}(A)$
	\item $U_{r}$ form a basis for $\mathcal{R}(A)$
\end{enumerate}
The Frobenius norm and spectral norm are tightly related to the SVD.
\[
	\|A\|_F = \sum_{i}\sigma_i^2
\]
\[
	\|A\|_2^2 = \sigma_1^2
\]
\section{Fundamentals of Optimization}
\begin{definition}
	The standard form of optimization is \[
		p^\star = \min_\V{x} f_0(\V{x}) \text{ such that } f_i(\V{x}) \leq 0
	\]
	\label{defn:optimization}
\end{definition}
\begin{itemize}
	\item The vector $\V{x}\in\mathbb{R}^n$ is known as the \textbf{decision variable}.
	\item The function $f_0:\mathbb{R}^n\to\mathbb{R}$ is the \textbf{objective}.
	\item The functions $f_i:\mathbb{R}^n\to\mathbb{R}$ is the \textbf{constraints}.
	\item $p^\star$ is the \textbf{optimal value}, and the $\V{x}^\star$ which achieves the optimal value is called the \textbf{optimizer}.
\end{itemize}
\begin{definition}
	The feasible set of an  optimization problem is \[
		\mathcal{X} = \{\V{x}\in\mathbb{R}^n:\ f_i(\V{x}) \leq 0 \}
	\]
	\label{defn:feasible-set}
\end{definition}
\begin{definition}
	A point $\V{x}$ is $\epsilon$-suboptimal if it is feasible and satisfies \[
		p^\star \leq f_0(\V{x}) \leq p^\star + \epsilon
	\]
	\label{defn:suboptimal}
\end{definition}
\begin{definition}
	An optimization problem is strictly feasible if $\exists \V{x}_0$ such that all constraints are strictly satisfied (i.e inequalities are strict inequalities, and equalities are satisfied).
	\label{defn:strictly-feasible}
\end{definition}
\subsection{Problem Transformations}
Sometimes, optimizations in a particular formulation do not admit themselves to be solved easily.
In this case, we can sometimes transform the problem into an easier one from which we can easily recover the solution to our original problem.
In many cases, we can introduce additional ``slack'' variable and constraints to massage the problem into a form which is easier to analyze.
\begin{theorem}[Epigraphic Constraints]
	$\min_\V{x} f_0(x)$ is equivalent to the problem with epigraphic constraints
	\[
		\min_{\V{x}, t} t \quad : \quad f_0(x) \leq t,
	\]
	\label{thm:epigraphic-const}
\end{theorem}
\cref{thm:epigraphic-const} works because by minimizing $t$, we are also minimizing how large $f_0(x)$ can get since $f_0(x) \leq t$, so at optimum, $f_0(x) = t$.
It can be helpful when $f_0(x) \leq t$ can be massaged further into constraints that are easier to deal with.
\begin{theorem}[Monotone Objective Transformation]
	Let $\Phi:\mathbb{R}\to\mathbb{R}$ be a continuous and strictly increasing function over a feasible set $\mathcal{X}$. Then \[
		\min_{\V{x}\in\mathcal{X}}f_0(\V{x}) \equiv \min_{\V{x}\in\mathcal{X}} \Phi(f_0(\V{x}))
	\]
	\label{thm:monotone-increasing}
\end{theorem}
\subsection{Robust Optimization}
For a ``nominal'' problem \[
	\min_\V{x} f_0(\V{x}) \quad : \quad \forall i\in[1,m],\ f_i(\V{x}) \leq 0,
\]
uncertainty can enter in the data used to create the $f_0$ and $f_i$.
It can also enter during decision time where the $\V{x}^\star$ which solves the optimization cannot be implemented exactly.
These uncertainties can create unstable solutions or degraded performance.
To make our optimization more robust to uncertainty, we add a new variable $\V{u}\in\mathcal{U}$.
\begin{definition}
	For a nominal optimization problem $\min_\V{x} f_0(\V{x})$ subject to $f_i(\V{x}) \leq 0$ for $i\in[1,m]$, the robust counterpart is \[
		\min_\V{x} \max_{\V{u}\in\mathcal{U}} f_0(\V{x}, \V{u}) \quad : \quad \forall i\in[1,m],\ f_i(\V{x}, \V{u}) \leq 0
	\]
	\label{defn:robust-counterpart}
\end{definition}
\section{Linear Algebraic Optimization}
Many optimization problems can be solved using the machinery of Linear Algebra.
These problems do not have inequality constraints or non-euclidean norms in the objective function.
\subsection{Projection}
The idea behind projection is to find the closest point in a set closest (with respect to particular norm) to a given point.
\begin{definition}
  Given a vector $\V{x}$ in inner product space $\mathcal{X}$ and a subspace $S\subseteq\mathcal{X}$, the projection of $\V{x}$ onto $S$ is given by
  \[
	\Pi_S(\V{x}) = \argmin_{\V{y}\in S}\|\V{y}-\V{x}\|
  \]
  where the norm is the one induced by the inner product.
  \label{defn:projection}
\end{definition}
\begin{theorem}
  There exists a unique vector $\V{x}^*\in S$ which solves
  \[
	\min_{\V{y}\in S} \|\V{y}-\V{x}\|.
  \]
  \label{thm:projection}
\end{theorem} It is necessary and sufficient for $\V{x}^*$ to be optimal that $(\V{x}-\V{x}^*)\perp S$.
The same condition applies when projecting onto an affine set.
\subsubsection{Matrix Pseudo-inverses}
\begin{definition}
	A pseudoinverse is a matrix $A^{\dagger}$ that satisfies:
	\[
		A A^\dagger A = A \quad A^\dagger A A^\dagger = A^\dagger \quad (AA^\dagger)^T = A A^\dagger \quad (A^\dagger A)^T = A^\dagger A
	\]
	\label{defn:pseduoinverse}
\end{definition}
There are several special cases of pseudoinverses.
\begin{enumerate}
	\item $A^\dagger = V_r \text{diag}\left(\frac{1}{\sigma_1},\cdots,\frac{1}{\sigma_r}\right)U_r^T$ is the Moore-Penrose Pseudo-inverse.
	\item When $A$ and non-singular, $A^\dagger = A^{-1}$.
	\item When $A$ is full column rank, $A^\dagger = (A^TA)^{-1}A^T$.
	\item When $A$ is full row rank, $A^{\dagger} = A^T(AA^T)^{-1}$
\end{enumerate}
The pseudo-inverses are useful because they can easily compute the projection of a vector onto a related subspace of $A$.
\begin{enumerate}
	\item $\argmin_{z\in\mathcal{R}(A)}\|\V{z}-\V{y}\|_2 = AA^\dagger \V{y}$
	\item $\argmin_{z\in\mathcal{R}(A)^\perp}\|\V{z}-\V{y}\|_2 = (I - AA^\dagger)\V{y}$
	\item $\argmin_{z\in\mathcal{N}(A)}\|\V{z}-\V{y}\|_2 = (I - A^\dagger A)\V{y}$
	\item $\argmin_{z\in\mathcal{N}(A)^\perp}\|\V{z}-\V{y}\|_2 = A^\dagger A\V{y}$
\end{enumerate}
\subsection{Explained Variance}
The Low Rank Approximation problem is to approximate a matrix $A$ with a rank $k$ matrix
\[
	\min_{A_k} \|A - A_k\|_F^2 \text{ such that rank}(A_k) = k.
\]
The solution to the low rank approximation problem is simply the first $k$ terms of the SVD:
\[
	A_K^\star = \sum_{i=1}^k \sigma_i\V{u}_i\V{v}^T_i.
\]
This is because the singular values give us a notion of how much of the Frobenius Norm (Total Variance) each dyad explains.
\[
	\eta = \frac{\|A_k\|_F^2}{\|A\|_F^2} = \frac{\sum_i^k \sigma_i^2}{\sum_i^r \sigma_i^2}
\]
\subsubsection{PCA}
Suppose we had a matrix containing $m$ data points in $\mathbb{R}^n$ (each data point is a column), and without loss of generality, assume this data is centered around 0 (i.e $\sum_i \V{x}_i = 0$).
The variance of this data along a particular direction $\V{z}$ is given by $\V{z}^TXX^T\V{z}$.
Principle Component Analysis is finding the directions $\V{z}$ such that the variance is maximized.
\[
	\max_{z\in\mathbb{R}^n} \V{z}^TXX^T\V{z} \text{ such that } \|\V{z}\|_2 = 1
\]
The left singular vector corresponding to the largest singular value of the $XX^T$ matrix is the optimizer of this problem, and the variance along this direction is $\sigma_1^2$.
If we wanted to find subsequent directions of maximal variance, they are just the left singular vectors corresponding to the largest singular values.
\subsection{Removing Constraints}
Following from the Fundmental Theorem of Linear Algebra, if $A\V{x}=\V{y}$ has a solution, then the set of solutions can be expressed as
\[
	S = \{\bar{\V{x}} + N\V{z}\}
\]
where $A\bar{\V{x}}=\V{y}$ and $N$ is a basis for $\mathcal{N}(A)$.
This means if we have a constrained optimization problem
\[
	\min_\V{x} f_0(\V{x}) \ : \ A\V{x} = \V{b},
\]
we can write an equivalent unconstrained problem \[
	\min_\V{z} f_0(\V{x}_0 + N\V{z})
\]
where $A\V{x}_0 = \V{b}$
\section{Convex Optimization}
\subsection{Convexity}
\begin{definition}
	A subset $C\in\mathbb{R}^n$ is convex if it contains the line segment between any two points in the set.
	\[
		\forall \V{x}_1, \V{x}_2\in C,\ \lambda\in[0, 1],\quad \lambda \V{x}_1+(1-\lambda)\V{x}_2 \in C
	\]
	\label{defn:convex-set}
\end{definition}
Convexity can be preserved by some operations.
\begin{theorem}
	If $C_1,\cdots,C_m$ are convex sets, then their intersection $C = \bigcap_{i=1,\cdots,m}C_i$ is also a convex set.
	\label{thm:convex-intersection}
\end{theorem}
\begin{theorem}
	If a map $f:\mathbb{R}^n\to\mathbb{R}^m$ is affine and $C \subset \mathbb{R}^n$ is convex, then $f(C) = \{ f(\V{x}): \V{x}\in C \}$ is convex.
	\label{thm:convex-affine}
\end{theorem}
\cref{thm:convex-intersection,thm:convex-affine} are important because they allow us to prove sets are convex using sets that we know are convex.
For example, \cref{thm:convex-affine} tells us that a projection of a convex set onto a subspace must also be convex since projection is a linear operator.
\begin{definition}
	A function $f:\mathbb{R}^n\to\mathbb{R}$ is convex if its domain is a convex set and $\forall \V{x}, \V{y}$ in the domain, $\lambda \in[0, 1]$,
	\[
		f(\lambda \V{x} + (1-\lambda)\V{y}) \leq \lambda f(\V{x}) + (1-\lambda)f(\V{y})
	\]
	\label{defn:convex-function}
\end{definition}
Loosely, convexity means that the function is bowl shaped since a line connecting any two points on the function is above the function itself.
A concave function is simply one where $-f$ is convex, and these appear like a ``hill''.
Because convex functions are bowl shaped, they must be $\infty$ outside their domain.
\begin{theorem}
	A function $f$ is convex if and only if its epigraph is a convex set.
	\label{thm:convex-function-epi}
\end{theorem}
Just like convex sets, some operations preserve convexity for functions.
\begin{theorem}
	If $f_i:\mathbb{R}^n\to\mathbb{R}$ are convex functions, then $f(\V{x}) = \sum_{i=1}^m\alpha_if_i(\V{x})$ where $\alpha_i\geq 0$ is also convex.
	\label{thm:convexity-nonegative-linear}
\end{theorem}
A similar property to \cref{thm:convex-affine} exists for convex functions.
\begin{theorem}
	If $f:\mathbb{R}^n\to\mathbb{R}$ is convex, then $g(\V{x}) = f(A\V{x}+b)$ is also convex.
	\label{thm:convex-func-affine}
\end{theorem}
We can also look at the first and second order derivatives to determine the convexity of a function.
\begin{theorem}
	If $f$ is differentiable, then $f$ is convex if and only if \[
		\forall \V{x}, \V{y},\quad f(\V{y}) \geq f(\V{x}) + \nabla_x^T (\V{y}-\V{x})
	\]
	\label{thm:first-order-condition}
\end{theorem}
\cref{thm:first-order-condition} can be understood geometrically by saying the graph of $f$ is bounded below everywhere by its tangent hyperplanes.
\begin{theorem}
	If $f$ is twice differentiable, then $f$ is convex if and only if the Hessian $\nabla^2$ is positive semi-definite everywhere.
	\label{thm:second-order-condition}
\end{theorem}
Geometrically, the second-order condition says that $f$ looks bowl-shaped.
\begin{theorem}
	A function $f$ is convex if and only if its restriction to any line $g(t)=f(\V{x}_0+t\V{v})$ is convex.
	\label{thm:convex-func-line}
\end{theorem}
\begin{theorem}
	If $(f_\alpha)_{\alpha\in\mathcal{A}}$ is a family of convex functions, then the pointwise maximum $f(\V{x}) = \max_{\alpha\in\mathcal{A}} f_\alpha(\V{x})$ is convex.
	\label{thm:pointwise-max}
\end{theorem}
Because of the nice geometry that convexity gives, optimization problems which involve convex functions and sets are reliably solveable.
\begin{definition}
	A convex optimization problem in standard form is \[
		p^* = \min_{\V{x}}f_0(\V{x}) : \quad \forall i\in[1,m], f_i(\V{x}) \leq 0, A\V{x} = \V{b}
	\]
	where $f_0, f_1, \cdots$ are convex functions and the equality constraints are affine.
	\label{defn:convex-prob}
\end{definition}
Since the constraints form a convex set, \cref{defn:convex-prob} is equivalent to minimizing a convex function over a convex set $\mathcal{X}$.
\begin{theorem}
	A locally optimal solution to a convex problem is also globally optimal, and this set $\mathcal{X}$ is convex.
	\label{thm:global-optima}
\end{theorem}
\cref{thm:global-optima} is why convex problems are nice to solve.
\subsection{Optimality}
When problems are convex, we can define conditions that any optimal solution must satisfy.
\begin{theorem}
	For a convex optimization problem with a differentiable objective function $f_0(\V{x})$ and feasible set $\mathcal{X}$,
	\[
		\V{x} \text{ is optimal } \Leftrightarrow \forall \V{y}\in\mathcal{X}, \nabla_xf_0(\V{x})^\top(\V{y}-\V{x}) \geq 0
	\]
	\label{thm:grad-optimality}
\end{theorem}
Since the gradient points in the direction of greatest increase, the dot product of the gradient with the different between any vector and the optimal solution being positive means other solutions will only increase the value of $f_0(\V{x})$. For unconstrained problems, we can make this condition even sharper.
\begin{theorem}
	In a convex unconstrained problem with a differentiable objective function $f_0(\V{x})$, $\V{x}$ is optimal if an only if $\nabla_xf_0(\V{x}) = \bs{0}$
	\label{thm:unconst-grad-optimality}
\end{theorem}
\subsection{Conic Programming}
Conic programming is the set of optimization problems which deal with variables constrained to a second-order cone.
\begin{definition}
	A n-dimensional second-order cone is the set \[
		\mathcal{K}_n = \{(\V{x}, t),\ \V{x}\in\mathbb{R}^n,\ t\in\mathbb{R}:\ \|\V{x}\|_2 \leq t\}
	\]
	\label{defn:soc}
\end{definition}
By Cauchy-Schwartz, $\|\V{x}\|_2 = \max_{\V{u}:\|\V{u}\|\leq 1} \V{u}^T\V{x} \leq t$.
This means that second order cones are convex sets since they are the intersection of half-spaces.
In spaces 3-dimensions and higher, we can rotate these cones.
\begin{definition}
	A rotated second order cone in $\mathbb{R}^{n+2}$ is the set \[
		\mathcal{K}_n^r = \{(\V{x}, y, z),\V{x}\in\mathbb{R}^n, y\in\mathbb{R}, z\in\mathbb{R}:\ \V{x}^T\V{x} \leq yz, y\geq 0, z \geq 0 \}.
	\]
	\label{defn:rot-soc}
\end{definition}
The rotated second-order cone can be interpreted as a rotation because the hyperbolic constraint $\|\V{x}\|_2^2\leq yz$ can be expressed equivalently as
\[
	\left\lVert\begin{bmatrix}2\V{x} \\ y - z\end{bmatrix}\right\rVert_2 \leq y+z.
\]
\begin{definition}
	The standard Second Order Cone Constraint is \[
		\|A\V{x}+\V{b}\|_2 \leq \V{c}^T\V{x} +d.
	\]
	\label{defn:soc-const}
\end{definition}
A SOC constraint will confine $\V{x}$ to a second order cone since if we let $\V{y} = A\V{x}+\V{b} \in \mathbb{R}^m$ and $t = \V{c}^T\V{x}+d$, then $(\V{y}, t)\in\mathcal{K}_m$.
\begin{definition}
	A second-order cone program in standard inequality form is given by
	\[
		\min \V{c}^T\V{x} \text{ such that } \|A_i\V{x}+\V{b}_i\|_2 \leq \V{c}_i^T\V{x}+d_i.
	\]
	\label{defn:soc-program}
\end{definition}
An SOC program is a convex problem since its objective is linear, and hence convex, and the SOC constraints are also convex.
\subsubsection{Quadratic Programming}
A special case of SOCPs are Quadratic Programs.
These programs have constraints and an objective function which can be expressed as a quadratic function.
In SOCP form, they look like
\begin{align*}
	\min_{\V{x}, t} &\quad \V{a}_0^T\V{x} + t\\
	\text{s.t: } & \left\lVert \begin{bmatrix}2Q_0^{\frac{1}{2}}\V{x}\\ t-1 \end{bmatrix}\right\rVert_2 \leq t+1\\
	& \left\lVert \begin{bmatrix}2Q_i^{\frac{1}{2}}\V{x}\\ b_i-\V{a}_i^T\V{x}-1 \end{bmatrix}\right\rVert_2 \leq b_i - \V{a}_i\V{x} + 1
\end{align*}
Since they are a special case of SOCPs, Quadratic Programs are also convex.
\begin{definition}
	The standard form of a quadratic constrained quadratic program is
	\[
		\min_\V{x} \V{x}^TQ_0\V{x} + \V{a}_0^T\V{x} \quad : \quad \forall i\in[1,m],\ \V{x}^TQ_i\V{x} + \V{a}_i^T\V{x} \leq b_i
	\]
	\label{defn:qcqp}
\end{definition}
To be a quadratic program, the matrix $H$ must be positive semi-definite.
If the $Q_i=0$ in the constraints, then we get a normal quadratic program.
\begin{definition}
	The standard form of a quadratic program is given by \[
		\min_\V{x}\frac{1}{2}\V{x}^TH\V{x} + \V{c}^T\V{x} \quad : \quad \forall i\in[1,m],\ \V{a}_i^T\V{x} \leq b_i
	\]
	\label{defn:quadratic-program}
\end{definition}
Its SOCP form looks like
\begin{align*}
	\min_{\V{x}, y} &\quad \V{c}^T\V{x} + y\\
	\text{s.t: } &\left\lVert \begin{bmatrix}2H^{\frac{1}{2}}\V{x} \\ y - 1 \end{bmatrix}\right\rVert_2 \leq y + 1,\\
	& \V{a}_i\V{x} \leq b_i
\end{align*}
In the special case where $H$ is positive definite and we have no constraints, then
\[
	\frac{1}{2}\V{x}^TH\V{x} + \V{c}^T\V{x} + d = \frac{1}{2}(\V{x} + H^{-1}\V{c})^TH(\V{x} + H^{-1}\V{c}) + d - (H^{-1}\V{c})^TH(H^{-1}\V{c})
\]
Thus \[
	\argmin_\V{x} \frac{1}{2}\V{x}^TH\V{x} + \V{c}^T\V{x} + d = -H^{-1}\V{c}
\]
\subsubsection{Linear Programming}
If the matrix in the objective function of a quadratic program is 0 (and there are no quadratic constraints), then the resulting objective and constraints are affine functions.
This is a linear program.
\begin{definition}
	The inequality form of a linear program is given by \[
		\min_\V{x} \V{c}^T\V{x} + d \quad : \quad \forall i\in[1,m],\ \V{a}_i^T\V{x} \leq b_i
	\]
	\label{defn:lp}
\end{definition}
Since linear program is a special case of a quadratic program, it can also be expressed as an SOCP.
\begin{align*}
	\min_\V{x} &\quad \V{c}^T\V{x}\\
	\text{s.t } &\quad \forall i\in[1,m],\ \|0\V{x} + 0\|_2 \leq b_i - \V{a}_i^T\V{x}
\end{align*}
Because of the constraints, the feasible set of a linear program is a polyhedron.
Thus linear programs are also convex.
\section{Duality}
\begin{definition}
	A primal optimization problem is given by
	\[
		p^* = \min_{\V{x}\in\mathbb{R}^n} f_0(\V{x}) : \forall i\in[1,m]\ f_i(\V{x}) \leq 0, \forall k\in[1,n]\ h_k(\V{x}) = 0
	\]
	\label{defn:primal}
\end{definition}
The primal problem is essentially the standard form of optimization.
There are no assumptions of convexity on any of the functions involved.
We can would like to express primal problems as a min-max optimization with no constraints.
\begin{definition}
	The Lagrangian $\mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu})$ using Lagrange Multipliers $\bs{\lambda}$ and $\bs{\mu}$ is given by 
	\[
		\mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu}) = f_0(\V{x}) + \sum_{i=1}^m\lambda_i f_i(\V{x}) + \sum_{k=1}^n \mu_i h_i(\V{x})
	\]
	\label{defn:lagrangian}
\end{definition}
The Lagrangian achieves the goal of removing the constraints in the min-max optimization
\[
	p^* = \min_{\V{x}\in\mathbb{R}^n}\max_{\bs{\lambda}\geq \bs{0}, \bs{\mu}} \mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu})
\]
This is true because if any inequality constraints are violated, then $f_i(\V{x}) \geq 0$, and the maximization could set $\lambda_i$ very large to make the overall problem $\infty$, and if any equality constraints are violated, then $h_k(\V{x}) \ne 0$, and the maximization would set $\mu_i$ to a very large number of the same sign as $h_k(\V{x})$ to make the overall problem $\infty$.
Thus the minimax problem is equivalent to the original problem.
At this point, it might be easier to solve the problem if the order of min and max were switched.
\begin{theorem}[Minimax Inequality]
	For any sets $X, Y$ and any function $F:X\times Y\to\mathbb{R}$ \[
		\min_{\V{x}\in X}\max_{\V{y}\in Y} F(\V{x}, \V{y}) \geq \max_{\V{y}\in Y}\min_{\V{x}\in X}F(\V{x}, \V{y})
	\]
	\label{thm:minimax}
\end{theorem}
\cref{thm:minimax} can be interpreted as a game where there is a minimizing player and a maximizing player.
If the maximizer goes first, it will always produce a higher score than if the minimizer goes first (unless they are equal).
We can now apply \cref{thm:minimax} to switch the $\min$ and $\max$ in our optimization with the Lagrangian.
\begin{theorem}[Weak Duality]
	\[
		\min_{\V{x}\in\mathbb{R}^n}\max_{\bs{\lambda}\geq \bs{0}, \bs{\mu}} \mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu}) \geq \max_{\bs{\lambda}\geq \bs{0}, \bs{\mu}} \min_{\V{x}\in\mathbb{R}^n} \mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu})
	\]

	\label{thm:weak-duality}
\end{theorem}
What weak duality does is convert our minimization problem to a maximization problem.
\begin{definition}
	The dual function of the primal problem is given by \[
		g(\bs{\lambda}, \bs{\mu}) = \min_{\V{x}} \mathcal{L}(\V{x}, \bs{\lambda}, \bs{\mu})
	\]
	\label{defn:dual-problem}
\end{definition}
Note that $g$ is a concave function because it is the pointwise minimum of functions that are affine in $\bs{\mu}$ and $\bs{\lambda}$.
A maximization of a concave function over a convex set is a convex problem, so the dual problem (minimizing $g$) is convex.
Thus duality achieves two primary purposes.
\begin{enumerate}
	\item It removes constraints, potentially making the problem easier to solve.
	\item It can turn a non-convex problems into a convex one.
\end{enumerate}
Even when there are no constraints, we can sometimes introduce constraints to leverage duality by adding slack variables that are equal to expressions in the objective.
\subsection{Strong Duality}
In some cases, duality gives not just a lower bound, but an exact value.
When this happens, we have \textbf{Strong Duality}.
\begin{theorem}[Sion's MiniMax Theorem]
	Let $X\subseteq\mathbb{R}^n$ be convex, and $Y\subseteq\mathbb{R}^m$ be bounded and closed (compact). Let $F:X \times Y \to \mathbb{R}$ be a function such that $\forall y,\ F(\cdot, y)$ is convex and continuous, and $\forall x,\ F(x, \cdot)$ is concave and continuous, then
	\[	
		\min_{\V{x}\in X}\max_{\V{y}\in Y} F(\V{x}, \V{y}) = \max_{\V{y}\in Y}\min_{\V{x}\in X}F(\V{x}, \V{y})
	\]
	\label{thm:scion-minimax}
\end{theorem}
If we focus on convex problems, then we can find conditions which indicate strong duality holds.
\begin{theorem}[Slater's Condition]
	If a convex optimization problem is strictly feasible, then strong duality holds
	\label{thm:slaters-condition}
\end{theorem}
Once we find a solution to the dual problem, then the solution to the primal problem is recovered by minimized $\mathcal{L}(\V{x}, \bs{\lambda}^*, \bs{\mu}^*)$ where $\bs{\lambda}^*,\bs{\mu}^*$ are the optimal dual variables, and if no such feasible point $\V{x}$ exists, then the primal itself is infeasible.
When searching for strong duality and an optimal solution $(\V{x}, \bs{\lambda}, \bs{\mu})$, it can be useful to consider particular conditions.
\begin{theorem}
	For a convex primal problem which is feasible and has a feasible dual where strong duality holds, a primal dual pair $(\V{x}, \bs{\lambda}, \bs{\mu})$ is optimal if and only if the KKT conditions are satisfied.

	1. \textbf{Primal Feasibility}: $\V{x}$ satisfies $\forall i\in[1,m],\ f_i(\V{x}) \leq 0$ and $\forall k\in[1,n],\ h_i(\V{x}) = 0$.
	
	2. \textbf{Dual Feasibility}: $\bs{\lambda} \geq \bs{0}$.
	
	3. \textbf{Complementary Slackness}: $\forall i\in[1,m],\ \lambda_if_i(\V{x}) = 0$
	
	4. \textbf{Lagrangian Stationarity}: If the lagrangian is differentiable, then \[
			\nabla_xf_0(\V{x}) +\sum_{i=1}^k\lambda_i\nabla_xf_i(\V{x}) + \sum_{k=1}^n\mu_ih_k(\V{x})=0
		\]
	\label{thm:kkt}
\end{theorem}
The complementary slackness requirement essentially says that if a primal constraint is slack ($f_i(\V{x}) < 0)$, then $\lambda_i=0$, and if $\lambda_i > 0$, then $f_i(\V{x}) = 0$.
\end{document}

