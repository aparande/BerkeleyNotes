\input{../../header.tex}
\begin{document}
\title{EECS127 Course Notes}
\author{Anmol Parande}
\date{Spring 2021 - Professor Laurent El Ghaoui}
\maketitle
\textbf{Disclaimer: }These notes reflect 127 when I took the course (Spring 2021). They may not accurately reflect current course content, so use at your own risk.
If you find any typos, errors, etc, please raise an issue on the \href{https://github.com/parandea17/BerkeleyNotes}{GitHub repository}.
\tableofcontents
\newpage
\section{Math Overview}
\subsection{Vectors}
\begin{definition}
  An affine set is one of the form $\mathcal{A}=\{ \V{x}\in\mathcal{X}:\ \V{x}=\V{v}+\V{x_0},\ \V{v}\in\mathcal{V}\}$ where $\mathcal{V}$ is a subspace of a vector space $\mathcal{X}$ and $x_0$ is a given point.
  \label{defn:affine-set}
\end{definition}
Notice that by \cref{defn:affine-set}, a subspace is simply an affine set containing the origin. Also notice that the dimension of an affine set $\mathcal{A}$ is the same as the dimension of $\mathcal{V}$.
For a given vector space, we can define a function which maps that vector to a real number.
\begin{definition}
  A norm on the vector space $\mathcal{X}$ is a function $\|\cdot\|:\mathcal{X}\rightarrow\mathbb{R}$ which satisfies $\|\V{x}\|\geq 0$ with equality if and only if $\V{x}=\bs{0}$, $\|\V{x}+\V{y}\|\leq\|\V{x}\|+\|\V{y}\|$, and $\|\alpha \V{x}\| = |\alpha|\|\V{x}\|$ for any scalar $\alpha$.
  \label{defn:norm}
\end{definition}
\begin{definition}
  The $l_p$ norms are defined by
  \[
	\|\V{x}\|_p=\left( \sum_{k=1}^n|x_k|^p \right)^{\frac{1}{p}},\ 1\leq p\leq \infty
  \]
  \label{defn:lp-norm}
\end{definition}
Notice that for $p=2$, we recover the Euclidean norm, and in the limit as $p\to\infty$, $\norm{\V{x}}_{\infty} = \max_k|x_k|$.
\begin{definition}
  An inner product on real vector space is a function that maps $\V{x},\V{y} \in \mathcal{X}$ to a non-negative scalar, is distributive, is commutative, and $\langle \V{x}, \V{x}, \rangle = 0 \Leftrightarrow \V{x}=0$.
  \label{defn:inner-product}
\end{definition}
Inner products induce a norm $\norm{\V{x}} = \sqrt{\langle \V{x}, \V{x} \rangle}$.
In $\mathbb{R}^n$, the standard inner product is $\V{x}^T\V{y}$.
The angle bewteen two vectors is given by
\[
  \cos\theta = \frac{\V{x}^T\V{y}}{\norm{\V{x}}_2\norm{\V{y}}_2}
\]
There are two other properties which use the standard inner product.
\begin{theorem}[Cauchy-Schwarz Inequality]
  \[
	|\V{x}^T\V{y}| \leq \norm{\V{x}}_2\norm{\V{y}}_2
  \]
  \label{thm:cauchy}
\end{theorem}
\begin{theorem}[Holder Inequality]
  \[
	|\V{x}^T\V{y}| \leq \sum_{k=1}^n |x_ky_k| \leq \norm{\V{x}}_p\norm{\V{y}}_q,\ p, q\geq 1 \text{ s.t } p^{-1}+q^{-1}=1.
  \]
  \label{thm:holder}
\end{theorem}
Notice that \cref{thm:holder} generalizes \cref{thm:cauchy}.
\subsubsection{Projection}
The idea behind projection is to find the closest point in a set closest (with respect to particular norm) to a given point.
\begin{definition}
  Given a vector $\V{x}$ in inner product space $\mathcal{X}$ and a subset $S\subseteq\mathcal{X}$, the projection of $\V{x}$ onto $S$ is given by
  \[
	\Pi_S(\V{x}) = \argmin_{\V{y}\in S}\norm{\V{y}-\V{x}}
  \]
  where the norm is the one induced by the inner product.
  \label{defn:projection}
\end{definition}
\begin{theorem}
  There exists a unique vector $\V{x}^*\in S$ which solves
  \[
	\min_{\V{y}\in S} \norm{\V{y}-\V{x}}.
  \]
  \label{thm:projection}
\end{theorem}
It is necessary and sufficient for $\V{x}^*$ to be optimal that $(\V{x}-\V{x}^*)\perp S$.
The same condition applies when projecting onto an affine set.
\subsection{Functions}
We consider functions to be of the form $f:\mathbb{R}^n\rightarrow\mathbb{R}$.
By contrast, a map is of the form $f:\mathbb{R}^n\rightarrow\mathbb{R}^m$.
The components of the map $f$ are the scalar valued functions $f_i$ that produce each component of a map.
\begin{definition}
  The graph of a function $f$ is the set of input-output pairs that $f$ can attain.
  \[
	\left\{ (x, f(x))\in \mathbb{R}^{n+1}:\ x\in\mathbb{R}^n \right\}
  \]
  \label{defn:graph}
\end{definition}
\begin{definition}
  The epigraph of a function is the set of input-output pairs that $f$ can achieve and anything above.
  \[
	\left\{ (x,t) \in \mathbb{R}^{n+1}:\ \V{x}\in\mathbb{R}^{n+1},\ t\geq f(x) \right\}
  \]
  \label{defn:epigraph}
\end{definition}
\begin{definition}
  The t-level set is the set of points that achieve exactly some value of $f$.
  \[
	\{ \V{x}\in\mathbb{R}^n:\ f(x)=t \}
  \]
  \label{defn:level-set}
\end{definition}
\begin{definition}
  The t-sublevel set of $f$ is the set of points achieving at most a value $t$.
  \[
	\{ x\in\mathbb{R}^n:\ f(x)\leq t \}
  \]
  \label{defn:sublevel-set}
\end{definition}
\begin{theorem}
  A function is linear if and only if it can be expressed as $f(\V{x}) = \V{a}^T\V{x}+b$ for some unique pair $(\V{a}, b)$.
  \label{thm:linear-ip}
\end{theorem}
An affine function is linear when $b=0$. A hyperplane is simply a level set of a linear function.
\begin{definition}
  The half-spaces are the regions of space which a hyper-plane separates.
  \[
	H_{\_} = \{ x: \V{a}^T\V{x}\leq b \} \qquad H_{+} = \{ x: \V{a}^T\V{x} > b \}
  \]
  \label{defn:halfspace}
\end{definition}
\begin{definition}
  The gradient of a function at a point $x$ where $f$ is differentiable is a column vector of first derviatives of $f$ with respsect to the components of $\V{x}$
  \[
	\nabla f(x) = \begin{bmatrix}
	  \frac{\partial f}{\partial x_1}\\
	  \vdots\\
	  \frac{\partial f}{\partial x_n}
	\end{bmatrix}
  \]
  \label{defn:gradient}
\end{definition}
The gradient is perpendicular to the level sets of $f$ and points from a point $\V{x}_0$ to higher values of the function.
In other words, it is the direction of steepest increase.
\subsection{Matrices}
Matrices define a linear map between an input space and an output space. 
Any linear map $f: \mathbb{R}^n \to \mathbb{R}^m$ can be represented by a matrix.
\begin{theorem}[Fundamental Theorem of Linear Algebra]
  For any matrix $A\in\mathbb{R}^{m\times n}$,
  \[
	\mathcal{N}(A) \oplus \mathcal{R}(A^T) = \mathbb{R}^n \qquad \mathcal{R}(A) \oplus \mathcal{N}(A^T) = \mathbb{R}^m.
  \]
  \label{thm:fundamental}
\end{theorem}
Similar to vectors, matrices can also have norms.
\begin{definition}
  A function $f: \mathbb{R}^{m\times n} \to \mathbb{R}$ is a matrix norm if 
  \[
	f(A) \geq 0 \quad f(A) = 0 \Leftrightarrow A = 0 \quad f(\alpha A) = |\alpha| f(A) \quad f(A+B) \leq f(A) + f(B)
  \]
  \label{defn:matrix-norm}
\end{definition}
\begin{definition}
  The Froebenius norm is the $l_2$ norm applied to all elements of the matrix.
  \[
	\|A\|_F = \sqrt{\text{trace} AA^T} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2}
  \]
  \label{defn:froebenius-norm}
\end{definition}
\begin{definition}
  The operator norms is defined as
  \[
	\|A\|_p = \max_{\V{u}\ne0} \frac{\|A\V{u}\|_p}{\|u\|_p}
  \]
  \label{defn:operator-norm}
\end{definition}
The operator norms characterize the maximum input-output gain of the matrix $A$ measured relative to some $l_p$ norm.
\subsubsection{QR Factorization}
\begin{definition}
  The QR factorization matrix are the orthogonal matrix Q and the upper triangular matrix R such that $A = QR$
  \label{defn:qr-factorization}
\end{definition}
An easy way to find the QR factorization of a matrix is to apply Graham Schmidt to the columns of the matrix and express the result in matrix form.
Suppose that our matrix $A$ is full rank (i.e its columns $\V{a}_i$ are linearly independent) and we have applied Graham-Schmidt to columns $\V{a}_{i+1}\cdots\V{a}_n$ to get orthogonal vectors $\V{q}_{i+1}\cdots\V{q}_{n}$.
Continuing the procedure, the ith orthogonal vector $\V{q}_i$ is
\[
  \V{\tilde{q}}_i = \V{a}_i - \sum_{k=i+1}^{n} (\V{q}_k^T \V{a}_k)\V{q}_k \qquad \V{q}_i = \frac{\V{\tilde{q}}_i}{\|\V{\tilde{q}}_i\|_2}.
\]
If we re-arrange this, to solve for $\V{a}_i$, we see that
\[
  \V{a}_i = \|\V{\tilde{q}}_i\|_2 \V{q}_i + \sum_{k=i+1}^{n} (\V{q}_k^T \V{a}_k)\V{q}_k.
\]
Putting this in matrix form, we can see that
\[
  \begin{bmatrix}
	\vrule & \vrule & & \vrule \\
	\V{a}_1 & \V{a}_2 & \cdots & \V{a}_{n}\\
	\vrule & \vrule & & \vrule 
  \end{bmatrix} = \begin{bmatrix}
	\vrule & \vrule & & \vrule \\
	\V{q}_1 & \V{q}_2 & \cdots & \V{q}_{n}\\
	\vrule & \vrule & & \vrule 
  \end{bmatrix} \begin{bmatrix}
	r_{11} & r_{12} & \cdots & r_{1n}\\
	0 & r_{22} & \cdots & r_{2n}\\
	\vdots & \ddots & \ddots & \vdots\\
	0 & \cdots & 0 & r_{nn}
  \end{bmatrix} \qquad r_{ij} = \V{a}_i^T\V{q_j}, r_{ii} = \|\V{\tilde{q}}_i\|_2.
\]

\end{document}

