\documentclass{article}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\begin{document}
    \title{Math 54 Course Notes}
    \author{Anmol Parande}
    \date{Fall 2018 - Professor Srivastava}
    \maketitle
    \section{Overview}
    These course notes reflect the content taught in UC Berkeley's Math 54 class taught by Professor Nikhil Srivastava in Fall 2018.
    They cover the basics of linear algebra and differential equations. The following documents explain each topic covered in the course in a way that groups similar concepts rather than directly following the order in which they were introduced in lecture.
    \section{Sets}
    Many of the proofs and notations which will appear later on will involve sets and set notation.
    \subsection{Basics of Sets}
    \begin{definition}
        A set is a a well-defined collection of objects
    \end{definition}
    For example, 
    $$A = \{x : x \in \mathbb{R} \text{ and } x \ge 0 \} $$
    defines the set of all real non-negative numbers.
    All sets have two forms: implicit and explicit. The explicit forms limit the number of variables you can have whereas implicit forms constrain the variables by equations.
    $$\text{Implicit: } A = \{ (x, y): x, y \in \mathbb{R} \text{ and } x+y=0 \}$$
    $$\text{Explicit: } A = \{ (x, -x): x \in \mathbb{R}\}$$

    In general, the explicit forms are more useful since we can choose any values for our variables and immediately generate elements in the set.

    \subsection[]{Set Operations and Notations}
    \begin{itemize}
        \item $\varnothing$ : The empty set
        \item U: The universal set
        \item $A \cap B: \{ x: x\in A \text{ and } x \in B \} $ (Intersection)
        \item $A \cup B: \{ x: x\in A \text{ or } x \in B \} $ (Union)
        \item $\bar{A} = \{ x:x \notin A \} $ (Complement)
        \item $A \subset B \Leftrightarrow x\in A \Rightarrow x \in B$ {A is a subset of B implies if x is in A, then x is in B}
        \item $ A \bigtimes B = \{(a, b): a\in A, b\in B\}$ : (Cartesian Product of 2 sets)
    \end{itemize}
    
    \begin{theorem}
        If $A \subseteq B$ and $B \subseteq A$, then $A = B$
    \end{theorem}

    \section{Linear Equations}
    Linear algebra is the study of systems of linear equations and linear transformations between vector spaces.
    Vector spaces and linear transformations will be covered later, but for now we will focus on systems of linear equations
    \subsection{Systems of Linear Equations}
    \begin{definition}
        A linear equation in n variables is an equation of form $\sum_{i=0}^{n}{a_ix_i} = b$ where $x_1,...x_n \text{ are variables and } a_1,...,a_n, b \in (\mathbb{R}, \mathbb{C})$
    \end{definition}
    Accordingly
    \begin{definition}
        A linear system in n variables is a finite collection of linear equations in those variables
    \end{definition}
    We say
    \begin{definition}
        $(s_1,...,s_n)$ is a solution to a linear system if $x_1=s_1, x_2=s_2, ..., x_n=s_n$ satisfies each equation in the linear system
    \end{definition}
    \begin{definition}
        The solution set to a linear system is all possible solutions
    \end{definition}
    For linear systems in particular, there are a limited number of possible forms that the solution set can take on.
    \begin{theorem}
        Given a linear system, the solution set is either empty, has one element, or has infinitely many elements.
    \end{theorem}
    \subsection{Gaussian Elimination}
    In order to solve large linear systems easily, we can use a process called Gaussian Elimination. The Gaussian Elimination algorithm makes it easy to find the solution sets of linear systems.
    When solving systems of equations, the most common method to solve them are variable elimination and substitution.
    Focusing on elimination, we can see that the only thing we need to really keep track of is the coefficients of each variable.
    To help of each track of these coefficients, each system of linear equations can be represented as a matrix called an \textbf{augmented matrix}
    \begin{definition}
        The augmented matrix contains the coefficients of each linear equation as its rows. The last column is what they are equal to.
        \[
            \left[
            \begin{array}{ccc|c}
            a_{11} & ... & a_{1n} & b_1 \\
            \vdots & \ddots & \vdots \\
            a_{m1} & ... & a_{mn} & b_n \\
            \end{array}
            \right]
        \]
    \end{definition}

    We can perform \textbf{row operations} on the augmented matrix. These row operations correspond to the steps you would take in variable elimination.
    \begin{definition}
        If $R_i$ and $R_j$ are rows in the augmented matrix, and $\alpha, \beta \in \mathbb{R}$, then a row operation is defined as $R_i=\alpha R_i +\beta R_j$
    \end{definition}
    In other words, replace $R_i$ with $\alpha R_i + \beta R_j$
    From this general form of row operation, we can get three specific row operations
    \begin{itemize}
        \item Scale a row: $\beta=0$
        \item Add one row to another: $\alpha R_i + \beta R_j$
        \item Swap two rows: $R_i=R_i+R_j; R_j=R_i-R_j; R_i=R_i-R_j$
    \end{itemize}
    The first two of these are synonymous with scaling and equation and adding two equations. The third is merely something useful when we solve systems with matrices.
    To solve linear systems, all we have to do is get matrices into one of the following forms.
    \begin{definition}
        A linear system is in row echelon form (REF) if it is of the form
        \[
            \left[
            \begin{array}{cccc|c}
            \Box & * & * & ... & * \\
            0 & \Box & * & ... & * \\
            0 & 0 & \Box & ... & * \\
            \vdots & \vdots & \ddots & \ddots & \vdots
            \end{array}
            \right]
        \] 
        where $*$ represents any number and $\Box$ represents a non-zero element.
        \begin{itemize}
            \item The leading nonzero entry in each row is a pivot. 
            \item Below and left of each pivot are only zeros. 
            \item Pivots move to the right as you go down the rows.
        \end{itemize}
    \end{definition}
    Remember that each row in the augmented matrix represents a single equation in the system. Thus in REF, the augmented matrix is equivalent to
    \[
        \begin{array}{c}
            \Box x_1 + *x_2 + ... = * \\
            \Box x_2 + *x_3 + ... = * \\
            \vdots \\
            \Box x_n = *
        \end{array}
    \]
    After REF, we can simplify even further
    \begin{definition}
        A linear system is in reduced row echelon form (REF) if it is of the form
        \[
            \left[
            \begin{array}{cccc|c}
            1 & 0 & 0 & ... & * \\
            0 & 1 & 0 & ... & * \\
            0 & 0 & 1 & ... & * \\
            \vdots & \vdots & \ddots & \ddots & \vdots
            \end{array}
            \right]
        \] 
        \begin{itemize}
            \item Each pivot is equal to one. 
            \item Entries above and left of pivots are 0.
        \end{itemize}
    \end{definition}
    When a matrix is in RREF, we can easily read off the solution to the linear system since each column with a pivot in it (a pivot variable) can be expressed in terms of the columns without any pivots (the free variables)
    For example, if our system in REF form is
    \[
        \left[
            \begin{array}{ccc|c}
                1 & 1 & 0 & 1 \\
                0 & 0 & 1 & 1 \\
                0 & 0 & 0 & 0 \\
            \end{array}
        \right]
    \], this is equivalent to 
    \[
        \begin{array}{c}
            x_1 = 1 - x_2 \\
            x_3 = 1
        \end{array}
    \]
    \begin{theorem}
        If an augmented matrix is in RREF, one of three things happen
        \begin{itemize}
            \item[1] There is a pivot in the last column $\Rightarrow$ No solution
            \item[2] There is a pivot in every column except the last one $\Rightarrow$ The last column gives the unique solution
            \item[3] No pivot is in the last column and at least one free variable exists $\rightarrow$ Infinitely many solutions
        \end{itemize}
    \end{theorem}
    For small matrices, it can be easy to get to REF and RREF. For larger matrices however, it will be useful to have an algorithmic process.
    \begin{definition}
        Gaussian Elimination is the algorithm for row reduction.
        \begin{itemize}
            \item[1] Identify the leftmost non-zero column
            \item[2] Choose a non-zero entry in the column and swap it to the topic
            \item[3] Create zeros below the pivot position by adding a multiple of the pivot row.
            \item[4] Repeat 1-3 on each submatrix below the pivot row and right of the pivot column until you get to REF
            \item[5] Scale the rows until pivots are equal to 1
            \item[6] Zero out entries above pivots from right to left
            \item[7] Repeat step 6 until you get to RREF      
        \end{itemize}
    \end{definition}
    Gaussian elimination is guaranteed to terminate since zeros are never destroyed once they are created. If you are wondering what the point of REF is since RREF immediately gives us a solution, then you should notice that REF has the same pivot and free variables as RREF.
    This means we can tell which type of solution we will get without fully completing Gaussian Elimination.

    \section{Vectors}
    In order to better understand what is happening with Gaussian elimination and solving linear systems, we'll introduce the concept of vectors.
    \begin{definition}
        A vector is a matrix with one column
        \[
        \left[
            \begin{array}{c}
                a_1\\
                a_2\\
                \vdots\\
                a_n
            \end{array}
        \right]
        \]
    \end{definition}
    There are two simple operations which we can do with vectors, scalar multiplication and addition
    \[ a\vec{v}=
        \left[
            \begin{array}{c}
                av_1\\
                av_2\\
                \vdots\\
                av_n
            \end{array}
        \right],
        \vec{v}+\vec{w} = \left[
            \begin{array}{c}
                v_1+w_1\\
                v_2+w_2\\
                \vdots\\
                v_n+w_n
            \end{array}
            \right]
        \]
    \begin{definition}
        Given vectors $v_1,\dots,v_n$ and scalars $c_1,/dots,c_n$, the vector $$\vec{y} = c_1\vec{v_1}+...+c_n\vec{v_n}$$ is a linear combination of the vectors
    \end{definition}
    The concept of a linear combination towards answering questions about linear systems.
    Asking if $\vec{y}$ is a linear combination of $\vec{v}, \vec{w}$ is equivalent to asking if $\exists c_1, c_2\in \mathbb{R}$ such that $\vec{y}=c_1\vec{v}+c_2\vec{w}$
    Asking if $\exists c_1, c_2\in \mathbb{R}$ is equivalent to asking if
    \[
        \left[
            \begin{array}{c}
                y_1\\
                y_2\\
                \vdots\\
                y_n
            \end{array}
        \right] = \left[
            \begin{array}{c}
                c_1v_1+c_2w_1\\
                \vdots\\
                c_nv_n+c_nw_n
            \end{array}
        \right]
        \]
    Notice that this is a linear system. Thus checking if $\vec{y}$ is a linear combination of $\vec{v}, \vec{w}$ is eqivalent to asking if
    \[
        \left[
            \begin{array}{cc|c}
                c_1 v_1 & c_2 w_1 & y_1\\
                \vdots & \vdots & \vdots\\
                c_1 v_n & c_2 w_n & y_n
            \end{array}
        \right]
        \] is consistent.
        \begin{theorem}
            $\vec{y}$ is a linear combination of $\vec{v_1},...,\vec{v_n}$ iff $\exists x_1,...,x_n$ such that $x_1\vec{v_1}+...+x_n\vec{v_n}=\vec{y}$
        \end{theorem}
        \begin{definition}
            Given $\vec{v_1}...\vec{v_n}$, the span is the set of all linear combinations of these vectors and is denoted $span{\vec{v_1}...\vec{v_n}}\subset \mathbb{R}^m$
        \end{definition}
        If the coefficient matrix of the linear combination can be put into RREF form, then the span of the n-dimensional vectors will be $\mathbb{R}^n$
        \begin{theorem}
            \label{Equivalence Theorem}
            $\vec{v_1},...,\vec{v_n}$ spans $\mathbb{R}^m$ iff $\vec{b}$ is a linear combination of the vectors $\forall b \in \mathbb{R}^m$.
            This is equivalent to saying the augmented matrix has a pivot in every row of the coefficient matrix.
        \end{theorem}
        \begin{corollary}
            If $\vec{v_1},...,\vec{v_p} \in \mathbb{R}^m$ and $p<m$, then $span{\vec{v_1}...\vec{v_n}} \ne \mathbb{R}^m$ 
            since the number of columns in the augmented matrix is less than the number of rows, so in REF, there won't be a pivot in every row.
        \end{corollary}
        Now that we have an idea of both matrices and vectors, we can reformulate linear systems in different terms
        \subsection{Matrix Multiplication and Linear Systems}
        \[
        \left[
            \begin{array}{ccc}
                a_{11} & ... & a_{1n}\\
                \vdots & \ddots & \vdots\\
                a_{m1} & ... & a_{mn}
            \end{array}
        \right]
        \left[
            \begin{array}{c}
                v_1\\
                \vdots\\
                v_n
            \end{array}
        \right] = 
        \left[
            \begin{array}{c}
                a_{11}v_1 + ... + a_{1n}v_n\\
                \vdots\\
                a_{m1} + ... + a_{mn}v_n
            \end{array}
        \right]
        \]
        This is the standard way of doing matrix multiplication, and notice that it forms a linear system.
        You can also think of a matrix as a series of vectors, in which case, matrix multiplication becomes
        \[
        \left[
            \begin{array}{ccc}
                \vline &  & \vline\\
                v_1 & ... & v_n\\
                \vline &  & \vline
            \end{array}
        \right]
        \left[
            \begin{array}{c}
                x_1\\
                \vdots\\
                x_n
            \end{array}
        \right] = 
        x_1 \vec{v_1} +...+x_n \vec{v_n}
        \]
        With these representations of linear systems we can formulate new categories of linear systems.
        \begin{definition}
            A linear system $A\vec{x}=\vec{b}$ is homogenous if $\vec{b}=\vec{0}$ and inhomogenous if $\vec{b} \ne \vec{0}$
        \end{definition}
        \begin{theorem}
            The solution set of a homogenous system $A\vec{x}=\vec{0}$ is the span of a set of vectors with coefficients corresponding to the free variables.
        \end{theorem}
        To see what this means consider the following example.
        \[
        \left[
            \begin{array}{cccc}
                1 & 1 & 0 & 1\\
                0 & 0 & 1 & 1\\
                0 & 0 & 0 & 0\\
                0 & 0 & 0 & 0
            \end{array}
        \right] \vec{x} = \vec{0}
        \]
        has the soluution
        \[
        \left\{
            \begin{array}{c}
                -x_2-x_4\\
                x_2\\
                -x_4\\
                x_4
            \end{array} : x_2, x_4 \in \mathbb{R}
            \right\} = \left\{
                \left[
                    \begin{array}{c}
                        -1\\
                        1\\
                        0\\
                        0
                    \end{array}
                    \right],
                    \left[
                        \begin{array}{c}
                            -1\\
                            0\\
                            -1\\
                            1
                        \end{array}
                        \right]
                \right\}
        \]
        
\end{document}