# State-Space Control {#sec:ss-control}

The basic idea behind state space control is to use the state of the
system in order to set the input. Namely, if we are given

$$\frac{d^{}\mathbf{x}}{dt^{}} = A\mathbf{x}+B\mathbf{u},$$

then we can let $$\mathbf{u} = \mathbf{r} - K\mathbf{x}$$. If we do
this, then the equivalent state-evolution equation becomes

$$\frac{d^{}\mathbf{x}}{dt^{}} = (A-BK)\mathbf{x}+B\mathbf{r}.$$

Notice that if our system is in phase variable form, then the controlled
state-evolution equation is

$$\frac{d^{}\mathbf{x}}{dt^{}} = \begin{bmatrix}     0 & 1 & 0 & 0 & \cdots\\     0 & 0 & 1 & 0 & \cdots\\     0 & 0 & \ddots & \ddots & \ddots\\     0 & 0 & \cdots & 0 & 1\\     -a_0-k_0 & -a_1-k_1 & \cdots & -a_{n-2}-k_{n-2} & -a_{n-1}k_{n-1}   \end{bmatrix}\mathbf{x} + B\mathbf{r}.$$

This makes it very convenient to place our poles where we want since the
last row of the $$A$$ matrix is also the coefficients of the
characteristic polynomial.

## Design by Transformation

Suppose we have a system

$$\frac{d^{}\mathbf{z}}{dt^{}} = A\mathbf{z}+B\mathbf{u} \qquad \mathbf{y} = C\mathbf{z}$$

which is not in phase variable form. To place it into phase variable
form, first assume that $$\mathbf{z} = P\mathbf{x}$$ for some invertible
matrix $$P$$.

$$P\frac{d^{}\mathbf{x}}{dt^{}} = AP\mathbf{x}+B\mathbf{u} \implies \frac{d^{}\mathbf{x}}{dt^{}} + P^{-1}B\mathbf{u} \qquad \mathbf{y} = CP\mathbf{x}.$$

Since our transformation is invertible, the controllability of the
system is unchanged, so

$$\mathcal{C}_x = \begin{bmatrix} P^{-1}B & P^{-1} AB \cdots P^{-1}A^{n-1}B \end{bmatrix} = P^{-1}\mathcal{C}_z.$$

Assuming the system is controllable,
$$P = \mathcal{C}_z\mathcal{C}_x^{-1}$$. Now we can apply state feedback
to the phase variable system.

$$\frac{d^{}\mathbf{x}}{dt^{}} = P^{-1}AP\mathbf{x}+P^{-1}B(-K\mathbf{x}+\mathbf{r}) = P^{-1}(AP-BK)P^{-1}\mathbb z + Br \implies K_z = K_zP^{-1}.$$

## Observers/State Estimators

When doing state feedback, we often don't have access to the states
themselves because we only have access to $$\mathbf{y}$$. In that case,
we can't use $$\mathbf{u}=\mathbf{r}-K\mathbf{x}$$ because we don't know
$$\mathbf{x}$$. One idea is to keep track of an estimated state
$$\hat{\mathbf{x}}$$ and estimated output $$\hat{\mathbf{y}}$$ which
follow the same system dynamics as the actual state and the actual
output and receive the same input. If $$A$$ is a stable matrix, then
$$\lim_{t\to\infty}e^{At}\mathbf{x}_0=0$$ where $$\mathbf{x}_0$$ is the
initial state of the system. This means that even if there is a
discrepancy between the estimated state and the true state in the
beginning, the estimate will match the true state after some time.

Suppose now that we want to control the error between the true state and
the estimated state $$\mathbf{e} = \mathbf{x} - \hat{\mathbf{x}}$$, so
we add a gain $$L$$ to the error in the outputs
$$\mathbf{y}-\hat{\mathbf{y}}$$.

$$\begin{aligned}   \frac{d^{}\hat{\mathbf{x}}}{dt^{}} &= A\hat{\mathbf{x}} + B\mathbf{u} + L(\mathbf{y}-\hat{\mathbf{y}}) = A\hat{\mathbf{x}}+B\mathbf{u}+LC(\mathbf{x}-\hat{\mathbf{x}})\\   \frac{d^{}\mathbf{e}}{dt^{}} &= \frac{d^{}(\mathbf{x}-\hat{\mathbf{x}})}{dt^{}} = A\mathbf{x}+B\mathbf{u} - \left[A\hat{\mathbf{x}}+B\mathbf{u}+LC(\mathbf{x}-\hat{\mathbf{x}})\right]\\   \therefore \frac{d^{}\mathbf{e}}{dt^{}} &= (A-LC)\mathbf{e}\end{aligned}$$

Thus we can design $$L$$ to get quick error convergence. Notice that if
our system is not observable, then we will not be able to place the
poles of the observer system where we want them to be.

Now, if we we do state feedback using the estimated state, then

$$\frac{d^{}\mathbf{x}}{dt^{}} = A\mathbf{x}+B(\mathbf{r}-K\hat{\mathbf{x}}) = (A-BK)\mathbf{x}+B(\mathbf{r}-K\mathbf{e}).$$

Looking at the combined system,

$$\begin{bmatrix} \frac{d^{}\mathbf{x}}{dt^{}} \\ \frac{d^{}\mathbf{e}}{dt^{}} \end{bmatrix}   = \begin{bmatrix} A-BK & -BK \\ 0 & A-LC \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \mathbf{e} \end{bmatrix}   + \begin{bmatrix} B \\ \boldsymbol{0} \end{bmatrix}\mathbf{r}.$$

Notice that the poles of this system are just the poles of the original
system and the poles of the observer system, so we can choose $$K$$ and
$$L$$ independently.

![Figure 12: State Observer System](.gitbook/assets/892f270d272babee7b989dde794eb0dd69190876.png)

## Integrators in State Feedback

Suppose we wanted to get rid of the steady state error using state-space
control. We would do this using an integrator over the error in the
observed outputs.

$$\mathbf{x}_N = \int_{0}^{t}(\vec{r}-\vec{y})dt.$$

If we treat this as a new state, its evolution will be

$$\frac{d^{}\mathbf{x}_N}{dt^{}} = r - C\mathbf{x}.$$

If our new control law is $$\mathbf{u} = -K\mathbf{x}+K_e\mathbf{x}_N$$,
then our new state-space equations are

$$\begin{aligned}   \begin{bmatrix} \frac{d^{}\mathbf{x}}{dt^{}} \\ \frac{d^{}\mathbf{x}_N}{dt^{}} \end{bmatrix}   = \begin{bmatrix} A & 0 \\ -C & 0 \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \mathbf{x}_N \end{bmatrix}   + \begin{bmatrix} B \\ \boldsymbol{0} \end{bmatrix} + \begin{bmatrix} \boldsymbol{0} \\ \boldsymbol{I} \end{bmatrix}\mathbf{r}.\end{aligned}$$

When we apply our feedback rule, we get

$$\begin{aligned}   \begin{bmatrix} \frac{d^{}\mathbf{x}}{dt^{}} \\ \frac{d^{}\mathbf{x}_N}{dt^{}} \end{bmatrix}   = \begin{bmatrix} A-BK & BK_e \\ -C & 0 \end{bmatrix} \begin{bmatrix} \mathbf{x} \\ \mathbf{x}_N \end{bmatrix}   + \begin{bmatrix} \boldsymbol{0} \\ \boldsymbol{I} \end{bmatrix}\mathbf{r}.\end{aligned}$$

## Linear Quadratic Regulator

Suppose we want to control our system to send the state to
$$\boldsymbol{0}$$ over an infinite time horizon using the input
$$-K\mathbf{x}$$. We want to do this by optimizing a cost function that
penalizes control effort and the state error. In particular, we want to
minimize the cost function

$$J = \int_{0}^{\infty}\mathbf{y}^TQ\mathbf{y} + \mathbf{u}^TR\mathbf{u} dt.$$

where $$R$$ and $$Q$$ are positve-semi-definite matrices (typically
diagonal) and determine how much we penalize the error and the control
effort.

